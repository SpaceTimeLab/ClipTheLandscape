{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b8bf8c3-89d4-4c6c-92bd-4711d52d9c47",
   "metadata": {},
   "source": [
    "# Train classifier heads\n",
    "\n",
    "- This notebook runs multiple experiments with Linear and MLP classification heads, and different modality combinations\n",
    "- Saves both the best head model and the logs for each combo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "40af647f-5727-48e2-bbe5-0360c7bc049c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset, Subset, TensorDataset, random_split\n",
    "from torchvision import transforms\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcf607ab-63c5-4166-8dd6-17b6ff81e355",
   "metadata": {},
   "source": [
    "## For consistent shuffling between experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "02a8a305-e0e6-4109-b798-168f5e2745ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "g = torch.Generator().manual_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "debf0c3b-6264-40db-ab4e-441a6946badc",
   "metadata": {},
   "source": [
    "## Set up a location embedder\n",
    "\n",
    "- We use rescaled lat/lon values which are centroids of ONS 1km grid\n",
    "- We need to define it before torch data loader so we can add locs to our datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c4f92ee-d4f9-4ebe-a9e6-56cb9244c495",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_cols = ['gridimage_id', 'title', 'grid_reference']\n",
    "metadata = pd.concat([\n",
    "    pd.read_csv('kaggle_data/metadata.csv', on_bad_lines='skip', usecols=metadata_cols),\n",
    "    pd.read_csv('kaggle_data/metadata-extra.csv', on_bad_lines='skip', encoding='latin1', usecols=metadata_cols)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bf0a7675-3b42-4e4e-a0f7-3dd7f863a59f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/w6/kmxyhb092_3gsgy99f9_t01r0000gn/T/ipykernel_30100/3917409025.py:10: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  geom_=lambda gdf_: gdf_.geometry.centroid,\n"
     ]
    }
   ],
   "source": [
    "grid2coords = (\n",
    "    gpd.read_file(\n",
    "        'misc/os_bng_grids.gpkg',\n",
    "        layer='1km_grid'\n",
    "    ).rename(\n",
    "        columns={'tile_name': 'grid_reference'}\n",
    "    )\n",
    "    .to_crs(4326)\n",
    "    .assign(\n",
    "        geom_=lambda gdf_: gdf_.geometry.centroid,\n",
    "        lat=lambda gdf_: gdf_.geom_.y,\n",
    "        lon=lambda gdf_: gdf_.geom_.x\n",
    "    )\n",
    "    .set_index('grid_reference')\n",
    "    [['lat', 'lon']].to_dict(orient='index')\n",
    ")\n",
    "\n",
    "img2grid = metadata.set_index('gridimage_id').grid_reference.to_dict()\n",
    "\n",
    "\n",
    "# UK bounds to rescale\n",
    "uk = {\n",
    "    'lat': {'min': 49.9, 'max': 61.9},\n",
    "    'lon': {'min': -8.6, 'max': 2.1}\n",
    "}\n",
    "\n",
    "def get_loc_emb(img_id):\n",
    "    \"\"\"Get rescaled lat/lon values\"\"\"\n",
    "    try:\n",
    "        coords_dict = grid2coords[img2grid[img_id]]\n",
    "        lat, lon = coords_dict['lat'], coords_dict['lon']\n",
    "\n",
    "        lat_norm = (lat - uk['lat']['min']) / (uk['lat']['max'] - uk['lat']['min'])\n",
    "        lon_norm = (lon - uk['lon']['min']) / (uk['lon']['max'] - uk['lon']['min'])\n",
    "    \n",
    "        return torch.tensor([lat_norm, lon_norm], dtype=torch.float32)\n",
    "    except:\n",
    "        return torch.tensor([0, 0], dtype=torch.float32)\n",
    "\n",
    "def add_location(batch):\n",
    "    # img_id, img_f, txt_f, loc_f, y\n",
    "    # batch is a list of items from full_ds[idx], e.g.\n",
    "    #   [(img_id0, img_f0, txt_f0, label0), (img_id1, img_f1, txt_f1, label1), …]\n",
    "    \n",
    "    img_ids, img_fs, txt_fs, loc_fs, labels = [], [], [], [], []\n",
    "    \n",
    "    for img_id, img_f, txt_f, label in batch:\n",
    "        img_ids.append(img_id)\n",
    "        img_fs.append(img_f)\n",
    "        txt_fs.append(txt_f)\n",
    "        loc_fs.append(get_loc_emb(img_id))\n",
    "        labels.append(label)\n",
    "\n",
    "    return (\n",
    "        torch.stack(img_ids),\n",
    "        torch.stack(img_fs),\n",
    "        torch.stack(txt_fs),\n",
    "        torch.stack(loc_fs),\n",
    "        torch.stack(labels)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a198398d-902b-4af7-a925-845e796b1996",
   "metadata": {},
   "source": [
    "## Load datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3fc13ed2-2880-4597-9566-a0acb44d3c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = torch.load('splits/dataset.pt', weights_only=False)\n",
    "train_idx = torch.load('splits/train_idx.pt', weights_only=False)\n",
    "val_idx = torch.load('splits/val_idx.pt', weights_only=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f825cc15-84ab-417b-89e0-308b30bc08fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = DataLoader(\n",
    "    Subset(ds, train_idx),\n",
    "    batch_size=4096, shuffle=True,  num_workers=0, generator=g, # consistent shuffling\n",
    "    collate_fn=add_location # Add location embeddings\n",
    ")\n",
    "\n",
    "val_dl = DataLoader(\n",
    "    Subset(ds, val_idx),\n",
    "    batch_size=4096, shuffle=False, num_workers=0,\n",
    "    collate_fn=add_location\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3392ad85-e5a2-48bf-a1ab-0ebaad7984da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samples in training: 517679\n",
      "Samples in validation: 129420\n"
     ]
    }
   ],
   "source": [
    "print(\"Samples in training:\", len(train_idx))\n",
    "print(\"Samples in validation:\", len(val_idx))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30a67e4c-d379-41fb-8dbb-5fab96f29b23",
   "metadata": {},
   "source": [
    "### Define MixUp helper function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b44c560e-111b-4db7-a1c3-1642578c745c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mixup(features, labels, alpha=0.4):\n",
    "    \"\"\"Return mixed features and mixed labels.\"\"\"\n",
    "    batch_size = features.size(0)\n",
    "    # sample mixing coefficient\n",
    "    lam = torch.distributions.Beta(alpha, alpha).sample((batch_size,)).to(features.device)\n",
    "    lam = torch.max(lam, 1-lam)           # ensure lam >= 0.5\n",
    "    lam = lam.view(batch_size, 1)         # shape [B,1]\n",
    "    \n",
    "    # shuffle batch\n",
    "    idx = torch.randperm(batch_size)\n",
    "    f2, y2 = features[idx], labels[idx]\n",
    "    \n",
    "    # mix\n",
    "    mixed_f = lam * features + (1-lam) * f2\n",
    "    mixed_y = lam * labels   + (1-lam) * y2   # soft labels\n",
    "    \n",
    "    return mixed_f, mixed_y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3e9655d-7f89-436f-8504-d25637bb0a38",
   "metadata": {},
   "source": [
    "## Define our main experiment function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d34c3823-ef56-4848-80e3-1bc1285a940c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(head, use_img, use_txt, use_loc, use_mixup):\n",
    "    \"\"\"\n",
    "    Runs a single experiment and saves best head/training log.\n",
    "    `head` is 'mlp' or 'linear'.\n",
    "    `use_img`, `use_txt`, `use_loc`, `use_mixup` are all booleans.\n",
    "    Two files will be generated: the best head in ./heads, and training logs in ./logs.\n",
    "    \"\"\"\n",
    "    \n",
    "    experiment_name = f'{head}{\"-img\" if use_img else \"\"}{\"-txt\" if use_txt else \"\"}{\"-loc\" if use_loc else \"\"}{\"-mixup\" if use_mixup else \"\"}'\n",
    "    num_labels = 49\n",
    "    device = 'cpu'\n",
    "    best_val = 0.0\n",
    "    best_f1 = 0.0\n",
    "    best_epoch = -1\n",
    "    patience = 10 # Stop if no improvement for 10 epochs\n",
    "    patience_counter = 0\n",
    "    epochs = 200\n",
    "    log_lines = []\n",
    "    loss_fn = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    # Calculate classifier input dimension\n",
    "    input_dim = sum([\n",
    "        512 if use_img else 0,\n",
    "        512 if use_txt else 0,\n",
    "        2 if use_loc else 0\n",
    "    ])\n",
    "\n",
    "    # Set up classifier head\n",
    "    if head == 'mlp':\n",
    "        head = nn.Sequential(\n",
    "            nn.Linear(input_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(256, num_labels)\n",
    "        ).to(device)\n",
    "        opt = AdamW(head.parameters(), lr=3e-3, weight_decay=1e-4)\n",
    "    elif head == 'linear':\n",
    "        head = nn.Linear(input_dim, num_labels).to(device)\n",
    "        opt = AdamW(head.parameters(), lr=5e-3, weight_decay=1e-5)\n",
    "    else:\n",
    "        print(\"Classification head must be `mlp` or `linear`!\")\n",
    "        return\n",
    "    \n",
    "    sch = CosineAnnealingLR(opt, T_max=epochs//4) # Learning schedule\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        head.train()\n",
    "        tot = 0\n",
    "        \n",
    "        for img_id, img_f, txt_f, loc_f, y in train_dl:\n",
    "            opt.zero_grad()\n",
    "    \n",
    "            # Combine modalities into a single embedding\n",
    "            fused_parts = []\n",
    "            if use_img:\n",
    "                fused_parts.append(img_f)\n",
    "            if use_txt:\n",
    "                fused_parts.append(txt_f)\n",
    "            if use_loc:\n",
    "                fused_parts.append(loc_f)\n",
    "                \n",
    "            fused = torch.cat(fused_parts, dim=1)\n",
    "\n",
    "            if use_mixup:\n",
    "                mixed_f, mixed_y = mixup(fused, y, alpha=0.2)\n",
    "                logits = head(mixed_f)\n",
    "                loss = loss_fn(logits, mixed_y)\n",
    "            else:\n",
    "                logits = head(fused)\n",
    "                loss = loss_fn(logits, y.float())\n",
    "            \n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            tot += loss.item() * fused.size(0)\n",
    "            \n",
    "        sch.step()\n",
    "    \n",
    "        # val‐subset accuracy\n",
    "        head.eval()\n",
    "        correct, total = 0, 0\n",
    "        all_preds, all_targets = [], [] # for per-class F-score\n",
    "        with torch.no_grad():\n",
    "            for img_id, img_f, txt_f, loc_f, y in val_dl:\n",
    "                \n",
    "                # Combine modalities into a single embedding\n",
    "                fused_parts = []\n",
    "                if use_img:\n",
    "                    fused_parts.append(img_f)\n",
    "                if use_txt:\n",
    "                    fused_parts.append(txt_f)\n",
    "                if use_loc:\n",
    "                    fused_parts.append(loc_f)\n",
    "                    \n",
    "                fused = torch.cat(fused_parts, dim=1)\n",
    "                \n",
    "                probs = torch.sigmoid(head(fused))\n",
    "                preds = (probs > 0.5)\n",
    "    \n",
    "                # Store for F1 calculation\n",
    "                all_preds.append(preds.cpu().numpy())\n",
    "                all_targets.append(y.cpu().numpy())\n",
    "            \n",
    "                # Subset accuracy: all 49 match?\n",
    "                correct += (preds.eq(y.bool()).all(dim=1).sum().item())\n",
    "                total   += fused.size(0)\n",
    "    \n",
    "        # Calculate average (macro) F1 score\n",
    "        all_preds = np.vstack(all_preds)\n",
    "        all_targets = np.vstack(all_targets)\n",
    "        macro_f1 = f1_score(all_targets, all_preds, average='macro', zero_division=0)\n",
    "        \n",
    "        train_loss = tot/len(train_idx)\n",
    "        val_acc = correct/total\n",
    "\n",
    "        log_line = f\"Epoch {epoch:02d} \\t train_loss {train_loss:.4f} \\t val_acc {val_acc:.4f} \\t macro_f1 {macro_f1:.4f}\"\n",
    "        log_lines.append(log_line)\n",
    "        print(log_line)\n",
    "    \n",
    "        if val_acc > best_val:\n",
    "            best_val = val_acc\n",
    "            best_f1 = macro_f1\n",
    "            best_epoch = epoch\n",
    "            patience_counter = 0\n",
    "            # Save best head\n",
    "            torch.save(head.state_dict(), f'heads/{experiment_name}.pth')\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "    \n",
    "        if patience_counter >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch} (no improvement for {patience} epochs)\")\n",
    "            break\n",
    "    \n",
    "    log_line = f\"======\\nBest epoch {best_epoch:02d} \\t val_acc {best_val:.4f} \\t macro_f1 {best_f1:.4f}\"\n",
    "    log_lines.append(log_line)\n",
    "    print(log_line)\n",
    "\n",
    "    # Save logs\n",
    "    with open(f'logs/{experiment_name}.txt', 'w') as f:\n",
    "        f.write('\\n'.join(log_lines))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34c63606-a517-446b-a1ed-b4e6beaa1566",
   "metadata": {},
   "source": [
    "## Run all combinations\n",
    "### Image embeddings only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "74663ef6-db57-4719-8576-43a6d128005b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00 \t train_loss 0.1291 \t val_acc 0.1717 \t macro_f1 0.3018\n",
      "Epoch 01 \t train_loss 0.0954 \t val_acc 0.1955 \t macro_f1 0.3570\n",
      "Epoch 02 \t train_loss 0.0927 \t val_acc 0.2021 \t macro_f1 0.3748\n",
      "Epoch 03 \t train_loss 0.0915 \t val_acc 0.2067 \t macro_f1 0.3836\n",
      "Epoch 04 \t train_loss 0.0907 \t val_acc 0.2099 \t macro_f1 0.3872\n",
      "Epoch 05 \t train_loss 0.0902 \t val_acc 0.2111 \t macro_f1 0.3923\n",
      "Epoch 06 \t train_loss 0.0898 \t val_acc 0.2133 \t macro_f1 0.3945\n",
      "Epoch 07 \t train_loss 0.0895 \t val_acc 0.2149 \t macro_f1 0.4011\n",
      "Epoch 08 \t train_loss 0.0893 \t val_acc 0.2159 \t macro_f1 0.3977\n",
      "Epoch 09 \t train_loss 0.0891 \t val_acc 0.2166 \t macro_f1 0.4030\n",
      "Epoch 10 \t train_loss 0.0889 \t val_acc 0.2176 \t macro_f1 0.4039\n",
      "Epoch 11 \t train_loss 0.0888 \t val_acc 0.2162 \t macro_f1 0.4039\n",
      "Epoch 12 \t train_loss 0.0887 \t val_acc 0.2189 \t macro_f1 0.4054\n",
      "Epoch 13 \t train_loss 0.0886 \t val_acc 0.2186 \t macro_f1 0.4069\n",
      "Epoch 14 \t train_loss 0.0885 \t val_acc 0.2190 \t macro_f1 0.4043\n",
      "Epoch 15 \t train_loss 0.0884 \t val_acc 0.2200 \t macro_f1 0.4071\n",
      "Epoch 16 \t train_loss 0.0883 \t val_acc 0.2214 \t macro_f1 0.4102\n",
      "Epoch 17 \t train_loss 0.0883 \t val_acc 0.2199 \t macro_f1 0.4070\n",
      "Epoch 18 \t train_loss 0.0882 \t val_acc 0.2209 \t macro_f1 0.4098\n",
      "Epoch 19 \t train_loss 0.0882 \t val_acc 0.2213 \t macro_f1 0.4097\n",
      "Epoch 20 \t train_loss 0.0881 \t val_acc 0.2207 \t macro_f1 0.4118\n",
      "Epoch 21 \t train_loss 0.0881 \t val_acc 0.2209 \t macro_f1 0.4093\n",
      "Epoch 22 \t train_loss 0.0880 \t val_acc 0.2216 \t macro_f1 0.4085\n",
      "Epoch 23 \t train_loss 0.0880 \t val_acc 0.2209 \t macro_f1 0.4080\n",
      "Epoch 24 \t train_loss 0.0880 \t val_acc 0.2201 \t macro_f1 0.4107\n",
      "Epoch 25 \t train_loss 0.0879 \t val_acc 0.2214 \t macro_f1 0.4092\n",
      "Epoch 26 \t train_loss 0.0879 \t val_acc 0.2219 \t macro_f1 0.4112\n",
      "Epoch 27 \t train_loss 0.0879 \t val_acc 0.2205 \t macro_f1 0.4090\n",
      "Epoch 28 \t train_loss 0.0878 \t val_acc 0.2214 \t macro_f1 0.4118\n",
      "Epoch 29 \t train_loss 0.0878 \t val_acc 0.2227 \t macro_f1 0.4118\n",
      "Epoch 30 \t train_loss 0.0878 \t val_acc 0.2223 \t macro_f1 0.4113\n",
      "Epoch 31 \t train_loss 0.0878 \t val_acc 0.2211 \t macro_f1 0.4096\n",
      "Epoch 32 \t train_loss 0.0877 \t val_acc 0.2220 \t macro_f1 0.4119\n",
      "Epoch 33 \t train_loss 0.0877 \t val_acc 0.2228 \t macro_f1 0.4129\n",
      "Epoch 34 \t train_loss 0.0877 \t val_acc 0.2218 \t macro_f1 0.4123\n",
      "Epoch 35 \t train_loss 0.0877 \t val_acc 0.2218 \t macro_f1 0.4111\n",
      "Epoch 36 \t train_loss 0.0877 \t val_acc 0.2224 \t macro_f1 0.4117\n",
      "Epoch 37 \t train_loss 0.0876 \t val_acc 0.2221 \t macro_f1 0.4113\n",
      "Epoch 38 \t train_loss 0.0876 \t val_acc 0.2223 \t macro_f1 0.4122\n",
      "Epoch 39 \t train_loss 0.0876 \t val_acc 0.2223 \t macro_f1 0.4116\n",
      "Epoch 40 \t train_loss 0.0876 \t val_acc 0.2218 \t macro_f1 0.4103\n",
      "Epoch 41 \t train_loss 0.0876 \t val_acc 0.2218 \t macro_f1 0.4111\n",
      "Epoch 42 \t train_loss 0.0876 \t val_acc 0.2222 \t macro_f1 0.4112\n",
      "Epoch 43 \t train_loss 0.0876 \t val_acc 0.2223 \t macro_f1 0.4119\n",
      "Early stopping at epoch 43 (no improvement for 10 epochs)\n",
      "======\n",
      "Best epoch 43 \t val_acc 0.2228 \t macro_f1 0.4129\n",
      "CPU times: user 15min 27s, sys: 23min 4s, total: 38min 31s\n",
      "Wall time: 5min 33s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "run_experiment('linear', use_img=True, use_txt=False, use_loc=False, use_mixup=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4382d659-2a66-4761-a4a1-eaaddd5489f8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00 \t train_loss 0.1354 \t val_acc 0.1767 \t macro_f1 0.3150\n",
      "Epoch 01 \t train_loss 0.1045 \t val_acc 0.1999 \t macro_f1 0.3730\n",
      "Epoch 02 \t train_loss 0.1021 \t val_acc 0.2079 \t macro_f1 0.3913\n",
      "Epoch 03 \t train_loss 0.1011 \t val_acc 0.2116 \t macro_f1 0.4009\n",
      "Epoch 04 \t train_loss 0.1006 \t val_acc 0.2140 \t macro_f1 0.4051\n",
      "Epoch 05 \t train_loss 0.1001 \t val_acc 0.2167 \t macro_f1 0.4096\n",
      "Epoch 06 \t train_loss 0.0999 \t val_acc 0.2177 \t macro_f1 0.4140\n",
      "Epoch 07 \t train_loss 0.0996 \t val_acc 0.2206 \t macro_f1 0.4168\n",
      "Epoch 08 \t train_loss 0.0994 \t val_acc 0.2212 \t macro_f1 0.4197\n",
      "Epoch 09 \t train_loss 0.0993 \t val_acc 0.2208 \t macro_f1 0.4190\n",
      "Epoch 10 \t train_loss 0.0991 \t val_acc 0.2219 \t macro_f1 0.4214\n",
      "Epoch 11 \t train_loss 0.0990 \t val_acc 0.2205 \t macro_f1 0.4186\n",
      "Epoch 12 \t train_loss 0.0989 \t val_acc 0.2230 \t macro_f1 0.4243\n",
      "Epoch 13 \t train_loss 0.0988 \t val_acc 0.2227 \t macro_f1 0.4235\n",
      "Epoch 14 \t train_loss 0.0988 \t val_acc 0.2226 \t macro_f1 0.4246\n",
      "Epoch 15 \t train_loss 0.0987 \t val_acc 0.2231 \t macro_f1 0.4250\n",
      "Epoch 16 \t train_loss 0.0987 \t val_acc 0.2229 \t macro_f1 0.4219\n",
      "Epoch 17 \t train_loss 0.0986 \t val_acc 0.2255 \t macro_f1 0.4298\n",
      "Epoch 18 \t train_loss 0.0986 \t val_acc 0.2246 \t macro_f1 0.4296\n",
      "Epoch 19 \t train_loss 0.0985 \t val_acc 0.2247 \t macro_f1 0.4280\n",
      "Epoch 20 \t train_loss 0.0985 \t val_acc 0.2246 \t macro_f1 0.4288\n",
      "Epoch 21 \t train_loss 0.0985 \t val_acc 0.2241 \t macro_f1 0.4286\n",
      "Epoch 22 \t train_loss 0.0984 \t val_acc 0.2248 \t macro_f1 0.4308\n",
      "Epoch 23 \t train_loss 0.0984 \t val_acc 0.2255 \t macro_f1 0.4292\n",
      "Epoch 24 \t train_loss 0.0984 \t val_acc 0.2262 \t macro_f1 0.4278\n",
      "Epoch 25 \t train_loss 0.0984 \t val_acc 0.2261 \t macro_f1 0.4298\n",
      "Epoch 26 \t train_loss 0.0983 \t val_acc 0.2266 \t macro_f1 0.4307\n",
      "Epoch 27 \t train_loss 0.0983 \t val_acc 0.2249 \t macro_f1 0.4268\n",
      "Epoch 28 \t train_loss 0.0983 \t val_acc 0.2266 \t macro_f1 0.4316\n",
      "Epoch 29 \t train_loss 0.0982 \t val_acc 0.2253 \t macro_f1 0.4295\n",
      "Epoch 30 \t train_loss 0.0982 \t val_acc 0.2254 \t macro_f1 0.4297\n",
      "Epoch 31 \t train_loss 0.0982 \t val_acc 0.2261 \t macro_f1 0.4296\n",
      "Epoch 32 \t train_loss 0.0982 \t val_acc 0.2258 \t macro_f1 0.4307\n",
      "Epoch 33 \t train_loss 0.0982 \t val_acc 0.2263 \t macro_f1 0.4301\n",
      "Epoch 34 \t train_loss 0.0981 \t val_acc 0.2262 \t macro_f1 0.4281\n",
      "Epoch 35 \t train_loss 0.0982 \t val_acc 0.2258 \t macro_f1 0.4300\n",
      "Epoch 36 \t train_loss 0.0981 \t val_acc 0.2255 \t macro_f1 0.4305\n",
      "Epoch 37 \t train_loss 0.0981 \t val_acc 0.2261 \t macro_f1 0.4303\n",
      "Epoch 38 \t train_loss 0.0981 \t val_acc 0.2260 \t macro_f1 0.4302\n",
      "Early stopping at epoch 38 (no improvement for 10 epochs)\n",
      "======\n",
      "Best epoch 38 \t val_acc 0.2266 \t macro_f1 0.4316\n",
      "CPU times: user 16min 2s, sys: 21min 5s, total: 37min 8s\n",
      "Wall time: 4min 59s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "run_experiment('linear', use_img=True, use_txt=False, use_loc=False, use_mixup=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a86f4c64-7c2c-44d0-b7e3-effba5afdf76",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00 \t train_loss 0.1437 \t val_acc 0.1482 \t macro_f1 0.2292\n",
      "Epoch 01 \t train_loss 0.1016 \t val_acc 0.1753 \t macro_f1 0.2791\n",
      "Epoch 02 \t train_loss 0.0984 \t val_acc 0.1856 \t macro_f1 0.3058\n",
      "Epoch 03 \t train_loss 0.0968 \t val_acc 0.1895 \t macro_f1 0.3152\n",
      "Epoch 04 \t train_loss 0.0959 \t val_acc 0.1940 \t macro_f1 0.3292\n",
      "Epoch 05 \t train_loss 0.0951 \t val_acc 0.1992 \t macro_f1 0.3356\n",
      "Epoch 06 \t train_loss 0.0946 \t val_acc 0.1985 \t macro_f1 0.3380\n",
      "Epoch 07 \t train_loss 0.0942 \t val_acc 0.2021 \t macro_f1 0.3430\n",
      "Epoch 08 \t train_loss 0.0938 \t val_acc 0.2042 \t macro_f1 0.3492\n",
      "Epoch 09 \t train_loss 0.0935 \t val_acc 0.2070 \t macro_f1 0.3532\n",
      "Epoch 10 \t train_loss 0.0932 \t val_acc 0.2056 \t macro_f1 0.3533\n",
      "Epoch 11 \t train_loss 0.0930 \t val_acc 0.2056 \t macro_f1 0.3525\n",
      "Epoch 12 \t train_loss 0.0928 \t val_acc 0.2078 \t macro_f1 0.3531\n",
      "Epoch 13 \t train_loss 0.0926 \t val_acc 0.2075 \t macro_f1 0.3554\n",
      "Epoch 14 \t train_loss 0.0925 \t val_acc 0.2099 \t macro_f1 0.3583\n",
      "Epoch 15 \t train_loss 0.0922 \t val_acc 0.2111 \t macro_f1 0.3626\n",
      "Epoch 16 \t train_loss 0.0921 \t val_acc 0.2107 \t macro_f1 0.3603\n",
      "Epoch 17 \t train_loss 0.0920 \t val_acc 0.2112 \t macro_f1 0.3624\n",
      "Epoch 18 \t train_loss 0.0918 \t val_acc 0.2126 \t macro_f1 0.3639\n",
      "Epoch 19 \t train_loss 0.0917 \t val_acc 0.2106 \t macro_f1 0.3609\n",
      "Epoch 20 \t train_loss 0.0916 \t val_acc 0.2144 \t macro_f1 0.3677\n",
      "Epoch 21 \t train_loss 0.0915 \t val_acc 0.2134 \t macro_f1 0.3667\n",
      "Epoch 22 \t train_loss 0.0913 \t val_acc 0.2127 \t macro_f1 0.3667\n",
      "Epoch 23 \t train_loss 0.0912 \t val_acc 0.2136 \t macro_f1 0.3696\n",
      "Epoch 24 \t train_loss 0.0910 \t val_acc 0.2155 \t macro_f1 0.3707\n",
      "Epoch 25 \t train_loss 0.0909 \t val_acc 0.2146 \t macro_f1 0.3704\n",
      "Epoch 26 \t train_loss 0.0908 \t val_acc 0.2152 \t macro_f1 0.3705\n",
      "Epoch 27 \t train_loss 0.0907 \t val_acc 0.2159 \t macro_f1 0.3696\n",
      "Epoch 28 \t train_loss 0.0906 \t val_acc 0.2163 \t macro_f1 0.3737\n",
      "Epoch 29 \t train_loss 0.0905 \t val_acc 0.2168 \t macro_f1 0.3753\n",
      "Epoch 30 \t train_loss 0.0904 \t val_acc 0.2172 \t macro_f1 0.3743\n",
      "Epoch 31 \t train_loss 0.0903 \t val_acc 0.2164 \t macro_f1 0.3716\n",
      "Epoch 32 \t train_loss 0.0902 \t val_acc 0.2162 \t macro_f1 0.3720\n",
      "Epoch 33 \t train_loss 0.0902 \t val_acc 0.2170 \t macro_f1 0.3735\n",
      "Epoch 34 \t train_loss 0.0901 \t val_acc 0.2182 \t macro_f1 0.3757\n",
      "Epoch 35 \t train_loss 0.0900 \t val_acc 0.2177 \t macro_f1 0.3753\n",
      "Epoch 36 \t train_loss 0.0900 \t val_acc 0.2177 \t macro_f1 0.3750\n",
      "Epoch 37 \t train_loss 0.0899 \t val_acc 0.2172 \t macro_f1 0.3739\n",
      "Epoch 38 \t train_loss 0.0899 \t val_acc 0.2181 \t macro_f1 0.3770\n",
      "Epoch 39 \t train_loss 0.0898 \t val_acc 0.2181 \t macro_f1 0.3756\n",
      "Epoch 40 \t train_loss 0.0898 \t val_acc 0.2187 \t macro_f1 0.3767\n",
      "Epoch 41 \t train_loss 0.0898 \t val_acc 0.2180 \t macro_f1 0.3765\n",
      "Epoch 42 \t train_loss 0.0898 \t val_acc 0.2185 \t macro_f1 0.3770\n",
      "Epoch 43 \t train_loss 0.0897 \t val_acc 0.2182 \t macro_f1 0.3760\n",
      "Epoch 44 \t train_loss 0.0897 \t val_acc 0.2183 \t macro_f1 0.3759\n",
      "Epoch 45 \t train_loss 0.0897 \t val_acc 0.2186 \t macro_f1 0.3768\n",
      "Epoch 46 \t train_loss 0.0897 \t val_acc 0.2185 \t macro_f1 0.3770\n",
      "Epoch 47 \t train_loss 0.0896 \t val_acc 0.2185 \t macro_f1 0.3768\n",
      "Epoch 48 \t train_loss 0.0897 \t val_acc 0.2185 \t macro_f1 0.3768\n",
      "Epoch 49 \t train_loss 0.0896 \t val_acc 0.2185 \t macro_f1 0.3768\n",
      "Epoch 50 \t train_loss 0.0896 \t val_acc 0.2185 \t macro_f1 0.3768\n",
      "Early stopping at epoch 50 (no improvement for 10 epochs)\n",
      "======\n",
      "Best epoch 50 \t val_acc 0.2187 \t macro_f1 0.3767\n",
      "CPU times: user 21min 22s, sys: 30min 20s, total: 51min 42s\n",
      "Wall time: 7min 4s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "run_experiment('mlp', use_img=True, use_txt=False, use_loc=False, use_mixup=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1ec0f703-6627-4711-a09e-e2733e3751e1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00 \t train_loss 0.1517 \t val_acc 0.1291 \t macro_f1 0.1927\n",
      "Epoch 01 \t train_loss 0.1115 \t val_acc 0.1655 \t macro_f1 0.2587\n",
      "Epoch 02 \t train_loss 0.1082 \t val_acc 0.1757 \t macro_f1 0.2823\n",
      "Epoch 03 \t train_loss 0.1067 \t val_acc 0.1809 \t macro_f1 0.2954\n",
      "Epoch 04 \t train_loss 0.1057 \t val_acc 0.1868 \t macro_f1 0.3064\n",
      "Epoch 05 \t train_loss 0.1052 \t val_acc 0.1908 \t macro_f1 0.3164\n",
      "Epoch 06 \t train_loss 0.1048 \t val_acc 0.1901 \t macro_f1 0.3180\n",
      "Epoch 07 \t train_loss 0.1043 \t val_acc 0.1941 \t macro_f1 0.3250\n",
      "Epoch 08 \t train_loss 0.1040 \t val_acc 0.1935 \t macro_f1 0.3237\n",
      "Epoch 09 \t train_loss 0.1037 \t val_acc 0.1970 \t macro_f1 0.3308\n",
      "Epoch 10 \t train_loss 0.1034 \t val_acc 0.1968 \t macro_f1 0.3331\n",
      "Epoch 11 \t train_loss 0.1033 \t val_acc 0.1998 \t macro_f1 0.3383\n",
      "Epoch 12 \t train_loss 0.1030 \t val_acc 0.2006 \t macro_f1 0.3401\n",
      "Epoch 13 \t train_loss 0.1029 \t val_acc 0.1993 \t macro_f1 0.3368\n",
      "Epoch 14 \t train_loss 0.1026 \t val_acc 0.1993 \t macro_f1 0.3386\n",
      "Epoch 15 \t train_loss 0.1025 \t val_acc 0.1996 \t macro_f1 0.3407\n",
      "Epoch 16 \t train_loss 0.1024 \t val_acc 0.2003 \t macro_f1 0.3424\n",
      "Epoch 17 \t train_loss 0.1022 \t val_acc 0.2008 \t macro_f1 0.3401\n",
      "Epoch 18 \t train_loss 0.1021 \t val_acc 0.2038 \t macro_f1 0.3458\n",
      "Epoch 19 \t train_loss 0.1018 \t val_acc 0.2051 \t macro_f1 0.3476\n",
      "Epoch 20 \t train_loss 0.1018 \t val_acc 0.2041 \t macro_f1 0.3496\n",
      "Epoch 21 \t train_loss 0.1017 \t val_acc 0.2045 \t macro_f1 0.3480\n",
      "Epoch 22 \t train_loss 0.1015 \t val_acc 0.2047 \t macro_f1 0.3487\n",
      "Epoch 23 \t train_loss 0.1015 \t val_acc 0.2056 \t macro_f1 0.3524\n",
      "Epoch 24 \t train_loss 0.1014 \t val_acc 0.2062 \t macro_f1 0.3554\n",
      "Epoch 25 \t train_loss 0.1012 \t val_acc 0.2064 \t macro_f1 0.3534\n",
      "Epoch 26 \t train_loss 0.1012 \t val_acc 0.2059 \t macro_f1 0.3537\n",
      "Epoch 27 \t train_loss 0.1010 \t val_acc 0.2035 \t macro_f1 0.3485\n",
      "Epoch 28 \t train_loss 0.1009 \t val_acc 0.2061 \t macro_f1 0.3537\n",
      "Epoch 29 \t train_loss 0.1009 \t val_acc 0.2055 \t macro_f1 0.3537\n",
      "Epoch 30 \t train_loss 0.1008 \t val_acc 0.2049 \t macro_f1 0.3543\n",
      "Epoch 31 \t train_loss 0.1007 \t val_acc 0.2075 \t macro_f1 0.3570\n",
      "Epoch 32 \t train_loss 0.1007 \t val_acc 0.2059 \t macro_f1 0.3550\n",
      "Epoch 33 \t train_loss 0.1006 \t val_acc 0.2079 \t macro_f1 0.3583\n",
      "Epoch 34 \t train_loss 0.1006 \t val_acc 0.2072 \t macro_f1 0.3564\n",
      "Epoch 35 \t train_loss 0.1005 \t val_acc 0.2078 \t macro_f1 0.3567\n",
      "Epoch 36 \t train_loss 0.1004 \t val_acc 0.2079 \t macro_f1 0.3573\n",
      "Epoch 37 \t train_loss 0.1004 \t val_acc 0.2080 \t macro_f1 0.3597\n",
      "Epoch 38 \t train_loss 0.1003 \t val_acc 0.2090 \t macro_f1 0.3591\n",
      "Epoch 39 \t train_loss 0.1003 \t val_acc 0.2091 \t macro_f1 0.3583\n",
      "Epoch 40 \t train_loss 0.1003 \t val_acc 0.2083 \t macro_f1 0.3584\n",
      "Epoch 41 \t train_loss 0.1002 \t val_acc 0.2086 \t macro_f1 0.3600\n",
      "Epoch 42 \t train_loss 0.1001 \t val_acc 0.2088 \t macro_f1 0.3597\n",
      "Epoch 43 \t train_loss 0.1002 \t val_acc 0.2095 \t macro_f1 0.3605\n",
      "Epoch 44 \t train_loss 0.1002 \t val_acc 0.2094 \t macro_f1 0.3604\n",
      "Epoch 45 \t train_loss 0.1001 \t val_acc 0.2089 \t macro_f1 0.3594\n",
      "Epoch 46 \t train_loss 0.1001 \t val_acc 0.2095 \t macro_f1 0.3606\n",
      "Epoch 47 \t train_loss 0.1001 \t val_acc 0.2094 \t macro_f1 0.3605\n",
      "Epoch 48 \t train_loss 0.1001 \t val_acc 0.2093 \t macro_f1 0.3603\n",
      "Epoch 49 \t train_loss 0.1000 \t val_acc 0.2094 \t macro_f1 0.3605\n",
      "Epoch 50 \t train_loss 0.1001 \t val_acc 0.2094 \t macro_f1 0.3605\n",
      "Epoch 51 \t train_loss 0.1000 \t val_acc 0.2094 \t macro_f1 0.3605\n",
      "Epoch 52 \t train_loss 0.1001 \t val_acc 0.2095 \t macro_f1 0.3603\n",
      "Epoch 53 \t train_loss 0.1001 \t val_acc 0.2093 \t macro_f1 0.3605\n",
      "Epoch 54 \t train_loss 0.1000 \t val_acc 0.2093 \t macro_f1 0.3607\n",
      "Epoch 55 \t train_loss 0.1001 \t val_acc 0.2096 \t macro_f1 0.3609\n",
      "Epoch 56 \t train_loss 0.1001 \t val_acc 0.2092 \t macro_f1 0.3601\n",
      "Epoch 57 \t train_loss 0.1000 \t val_acc 0.2094 \t macro_f1 0.3603\n",
      "Epoch 58 \t train_loss 0.1001 \t val_acc 0.2091 \t macro_f1 0.3591\n",
      "Epoch 59 \t train_loss 0.1001 \t val_acc 0.2092 \t macro_f1 0.3603\n",
      "Epoch 60 \t train_loss 0.1000 \t val_acc 0.2098 \t macro_f1 0.3613\n",
      "Epoch 61 \t train_loss 0.1001 \t val_acc 0.2095 \t macro_f1 0.3607\n",
      "Epoch 62 \t train_loss 0.1001 \t val_acc 0.2090 \t macro_f1 0.3603\n",
      "Epoch 63 \t train_loss 0.1001 \t val_acc 0.2103 \t macro_f1 0.3640\n",
      "Epoch 64 \t train_loss 0.1001 \t val_acc 0.2107 \t macro_f1 0.3637\n",
      "Epoch 65 \t train_loss 0.1000 \t val_acc 0.2105 \t macro_f1 0.3633\n",
      "Epoch 66 \t train_loss 0.1000 \t val_acc 0.2116 \t macro_f1 0.3642\n",
      "Epoch 67 \t train_loss 0.1000 \t val_acc 0.2109 \t macro_f1 0.3646\n",
      "Epoch 68 \t train_loss 0.0999 \t val_acc 0.2105 \t macro_f1 0.3649\n",
      "Epoch 69 \t train_loss 0.0999 \t val_acc 0.2105 \t macro_f1 0.3668\n",
      "Epoch 70 \t train_loss 0.0998 \t val_acc 0.2108 \t macro_f1 0.3662\n",
      "Epoch 71 \t train_loss 0.0999 \t val_acc 0.2118 \t macro_f1 0.3656\n",
      "Epoch 72 \t train_loss 0.0998 \t val_acc 0.2122 \t macro_f1 0.3666\n",
      "Epoch 73 \t train_loss 0.0998 \t val_acc 0.2107 \t macro_f1 0.3620\n",
      "Epoch 74 \t train_loss 0.0997 \t val_acc 0.2122 \t macro_f1 0.3673\n",
      "Epoch 75 \t train_loss 0.0997 \t val_acc 0.2108 \t macro_f1 0.3688\n",
      "Epoch 76 \t train_loss 0.0997 \t val_acc 0.2139 \t macro_f1 0.3700\n",
      "Epoch 77 \t train_loss 0.0996 \t val_acc 0.2126 \t macro_f1 0.3704\n",
      "Epoch 78 \t train_loss 0.0997 \t val_acc 0.2134 \t macro_f1 0.3687\n",
      "Epoch 79 \t train_loss 0.0996 \t val_acc 0.2141 \t macro_f1 0.3750\n",
      "Epoch 80 \t train_loss 0.0996 \t val_acc 0.2143 \t macro_f1 0.3699\n",
      "Epoch 81 \t train_loss 0.0995 \t val_acc 0.2156 \t macro_f1 0.3755\n",
      "Epoch 82 \t train_loss 0.0996 \t val_acc 0.2111 \t macro_f1 0.3666\n",
      "Epoch 83 \t train_loss 0.0995 \t val_acc 0.2125 \t macro_f1 0.3741\n",
      "Epoch 84 \t train_loss 0.0995 \t val_acc 0.2130 \t macro_f1 0.3720\n",
      "Epoch 85 \t train_loss 0.0996 \t val_acc 0.2168 \t macro_f1 0.3763\n",
      "Epoch 86 \t train_loss 0.0995 \t val_acc 0.2121 \t macro_f1 0.3720\n",
      "Epoch 87 \t train_loss 0.0994 \t val_acc 0.2140 \t macro_f1 0.3772\n",
      "Epoch 88 \t train_loss 0.0995 \t val_acc 0.2132 \t macro_f1 0.3738\n",
      "Epoch 89 \t train_loss 0.0995 \t val_acc 0.2139 \t macro_f1 0.3748\n",
      "Epoch 90 \t train_loss 0.0994 \t val_acc 0.2162 \t macro_f1 0.3762\n",
      "Epoch 91 \t train_loss 0.0994 \t val_acc 0.2126 \t macro_f1 0.3724\n",
      "Epoch 92 \t train_loss 0.0994 \t val_acc 0.2150 \t macro_f1 0.3788\n",
      "Epoch 93 \t train_loss 0.0994 \t val_acc 0.2158 \t macro_f1 0.3780\n",
      "Epoch 94 \t train_loss 0.0994 \t val_acc 0.2154 \t macro_f1 0.3772\n",
      "Epoch 95 \t train_loss 0.0994 \t val_acc 0.2145 \t macro_f1 0.3770\n",
      "Early stopping at epoch 95 (no improvement for 10 epochs)\n",
      "======\n",
      "Best epoch 95 \t val_acc 0.2168 \t macro_f1 0.3763\n",
      "CPU times: user 38min 41s, sys: 1h 3min 10s, total: 1h 41min 52s\n",
      "Wall time: 14min 3s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "run_experiment('mlp', use_img=True, use_txt=False, use_loc=False, use_mixup=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9041c1c9-57e2-450b-9d54-2e4e2154704f",
   "metadata": {},
   "source": [
    "### Title embeddings only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4b15602d-56a1-4906-8a94-3ab50be87c65",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00 \t train_loss 0.1364 \t val_acc 0.2051 \t macro_f1 0.3694\n",
      "Epoch 01 \t train_loss 0.0946 \t val_acc 0.2393 \t macro_f1 0.4402\n",
      "Epoch 02 \t train_loss 0.0911 \t val_acc 0.2517 \t macro_f1 0.4635\n",
      "Epoch 03 \t train_loss 0.0896 \t val_acc 0.2564 \t macro_f1 0.4733\n",
      "Epoch 04 \t train_loss 0.0887 \t val_acc 0.2603 \t macro_f1 0.4784\n",
      "Epoch 05 \t train_loss 0.0881 \t val_acc 0.2639 \t macro_f1 0.4846\n",
      "Epoch 06 \t train_loss 0.0877 \t val_acc 0.2658 \t macro_f1 0.4892\n",
      "Epoch 07 \t train_loss 0.0873 \t val_acc 0.2679 \t macro_f1 0.4898\n",
      "Epoch 08 \t train_loss 0.0871 \t val_acc 0.2681 \t macro_f1 0.4904\n",
      "Epoch 09 \t train_loss 0.0868 \t val_acc 0.2688 \t macro_f1 0.4932\n",
      "Epoch 10 \t train_loss 0.0867 \t val_acc 0.2691 \t macro_f1 0.4937\n",
      "Epoch 11 \t train_loss 0.0865 \t val_acc 0.2709 \t macro_f1 0.4967\n",
      "Epoch 12 \t train_loss 0.0864 \t val_acc 0.2720 \t macro_f1 0.4983\n",
      "Epoch 13 \t train_loss 0.0863 \t val_acc 0.2720 \t macro_f1 0.4992\n",
      "Epoch 14 \t train_loss 0.0862 \t val_acc 0.2726 \t macro_f1 0.4993\n",
      "Epoch 15 \t train_loss 0.0861 \t val_acc 0.2716 \t macro_f1 0.4988\n",
      "Epoch 16 \t train_loss 0.0860 \t val_acc 0.2733 \t macro_f1 0.4999\n",
      "Epoch 17 \t train_loss 0.0860 \t val_acc 0.2733 \t macro_f1 0.4990\n",
      "Epoch 18 \t train_loss 0.0859 \t val_acc 0.2738 \t macro_f1 0.4985\n",
      "Epoch 19 \t train_loss 0.0859 \t val_acc 0.2737 \t macro_f1 0.5002\n",
      "Epoch 20 \t train_loss 0.0858 \t val_acc 0.2742 \t macro_f1 0.5016\n",
      "Epoch 21 \t train_loss 0.0858 \t val_acc 0.2741 \t macro_f1 0.5018\n",
      "Epoch 22 \t train_loss 0.0858 \t val_acc 0.2744 \t macro_f1 0.5014\n",
      "Epoch 23 \t train_loss 0.0857 \t val_acc 0.2741 \t macro_f1 0.5017\n",
      "Epoch 24 \t train_loss 0.0857 \t val_acc 0.2746 \t macro_f1 0.5030\n",
      "Epoch 25 \t train_loss 0.0857 \t val_acc 0.2751 \t macro_f1 0.5034\n",
      "Epoch 26 \t train_loss 0.0856 \t val_acc 0.2757 \t macro_f1 0.5037\n",
      "Epoch 27 \t train_loss 0.0856 \t val_acc 0.2751 \t macro_f1 0.5031\n",
      "Epoch 28 \t train_loss 0.0856 \t val_acc 0.2747 \t macro_f1 0.5033\n",
      "Epoch 29 \t train_loss 0.0855 \t val_acc 0.2749 \t macro_f1 0.5025\n",
      "Epoch 30 \t train_loss 0.0855 \t val_acc 0.2759 \t macro_f1 0.5026\n",
      "Epoch 31 \t train_loss 0.0855 \t val_acc 0.2755 \t macro_f1 0.5032\n",
      "Epoch 32 \t train_loss 0.0855 \t val_acc 0.2755 \t macro_f1 0.5047\n",
      "Epoch 33 \t train_loss 0.0855 \t val_acc 0.2756 \t macro_f1 0.5029\n",
      "Epoch 34 \t train_loss 0.0854 \t val_acc 0.2756 \t macro_f1 0.5031\n",
      "Epoch 35 \t train_loss 0.0854 \t val_acc 0.2750 \t macro_f1 0.5032\n",
      "Epoch 36 \t train_loss 0.0854 \t val_acc 0.2759 \t macro_f1 0.5034\n",
      "Epoch 37 \t train_loss 0.0854 \t val_acc 0.2757 \t macro_f1 0.5034\n",
      "Epoch 38 \t train_loss 0.0854 \t val_acc 0.2755 \t macro_f1 0.5046\n",
      "Epoch 39 \t train_loss 0.0854 \t val_acc 0.2761 \t macro_f1 0.5044\n",
      "Epoch 40 \t train_loss 0.0854 \t val_acc 0.2759 \t macro_f1 0.5041\n",
      "Epoch 41 \t train_loss 0.0853 \t val_acc 0.2761 \t macro_f1 0.5046\n",
      "Epoch 42 \t train_loss 0.0853 \t val_acc 0.2760 \t macro_f1 0.5044\n",
      "Epoch 43 \t train_loss 0.0853 \t val_acc 0.2756 \t macro_f1 0.5040\n",
      "Epoch 44 \t train_loss 0.0853 \t val_acc 0.2761 \t macro_f1 0.5040\n",
      "Epoch 45 \t train_loss 0.0853 \t val_acc 0.2762 \t macro_f1 0.5044\n",
      "Epoch 46 \t train_loss 0.0853 \t val_acc 0.2762 \t macro_f1 0.5045\n",
      "Epoch 47 \t train_loss 0.0853 \t val_acc 0.2762 \t macro_f1 0.5045\n",
      "Epoch 48 \t train_loss 0.0853 \t val_acc 0.2762 \t macro_f1 0.5044\n",
      "Epoch 49 \t train_loss 0.0853 \t val_acc 0.2763 \t macro_f1 0.5045\n",
      "Epoch 50 \t train_loss 0.0853 \t val_acc 0.2763 \t macro_f1 0.5045\n",
      "Epoch 51 \t train_loss 0.0853 \t val_acc 0.2762 \t macro_f1 0.5045\n",
      "Epoch 52 \t train_loss 0.0853 \t val_acc 0.2763 \t macro_f1 0.5045\n",
      "Epoch 53 \t train_loss 0.0853 \t val_acc 0.2761 \t macro_f1 0.5043\n",
      "Epoch 54 \t train_loss 0.0853 \t val_acc 0.2761 \t macro_f1 0.5043\n",
      "Epoch 55 \t train_loss 0.0853 \t val_acc 0.2763 \t macro_f1 0.5046\n",
      "Epoch 56 \t train_loss 0.0853 \t val_acc 0.2761 \t macro_f1 0.5045\n",
      "Epoch 57 \t train_loss 0.0853 \t val_acc 0.2761 \t macro_f1 0.5045\n",
      "Epoch 58 \t train_loss 0.0853 \t val_acc 0.2759 \t macro_f1 0.5048\n",
      "Epoch 59 \t train_loss 0.0853 \t val_acc 0.2762 \t macro_f1 0.5045\n",
      "Epoch 60 \t train_loss 0.0853 \t val_acc 0.2759 \t macro_f1 0.5049\n",
      "Epoch 61 \t train_loss 0.0853 \t val_acc 0.2763 \t macro_f1 0.5050\n",
      "Epoch 62 \t train_loss 0.0853 \t val_acc 0.2759 \t macro_f1 0.5052\n",
      "Epoch 63 \t train_loss 0.0854 \t val_acc 0.2762 \t macro_f1 0.5047\n",
      "Epoch 64 \t train_loss 0.0854 \t val_acc 0.2764 \t macro_f1 0.5057\n",
      "Epoch 65 \t train_loss 0.0854 \t val_acc 0.2761 \t macro_f1 0.5054\n",
      "Epoch 66 \t train_loss 0.0854 \t val_acc 0.2765 \t macro_f1 0.5063\n",
      "Epoch 67 \t train_loss 0.0854 \t val_acc 0.2768 \t macro_f1 0.5058\n",
      "Epoch 68 \t train_loss 0.0854 \t val_acc 0.2760 \t macro_f1 0.5049\n",
      "Epoch 69 \t train_loss 0.0854 \t val_acc 0.2762 \t macro_f1 0.5036\n",
      "Epoch 70 \t train_loss 0.0854 \t val_acc 0.2764 \t macro_f1 0.5032\n",
      "Epoch 71 \t train_loss 0.0854 \t val_acc 0.2763 \t macro_f1 0.5043\n",
      "Epoch 72 \t train_loss 0.0854 \t val_acc 0.2757 \t macro_f1 0.5038\n",
      "Epoch 73 \t train_loss 0.0854 \t val_acc 0.2767 \t macro_f1 0.5062\n",
      "Epoch 74 \t train_loss 0.0854 \t val_acc 0.2770 \t macro_f1 0.5059\n",
      "Epoch 75 \t train_loss 0.0854 \t val_acc 0.2759 \t macro_f1 0.5048\n",
      "Epoch 76 \t train_loss 0.0854 \t val_acc 0.2761 \t macro_f1 0.5045\n",
      "Epoch 77 \t train_loss 0.0855 \t val_acc 0.2762 \t macro_f1 0.5045\n",
      "Epoch 78 \t train_loss 0.0855 \t val_acc 0.2771 \t macro_f1 0.5063\n",
      "Epoch 79 \t train_loss 0.0855 \t val_acc 0.2778 \t macro_f1 0.5062\n",
      "Epoch 80 \t train_loss 0.0855 \t val_acc 0.2767 \t macro_f1 0.5068\n",
      "Epoch 81 \t train_loss 0.0855 \t val_acc 0.2768 \t macro_f1 0.5066\n",
      "Epoch 82 \t train_loss 0.0855 \t val_acc 0.2771 \t macro_f1 0.5079\n",
      "Epoch 83 \t train_loss 0.0855 \t val_acc 0.2763 \t macro_f1 0.5046\n",
      "Epoch 84 \t train_loss 0.0855 \t val_acc 0.2775 \t macro_f1 0.5069\n",
      "Epoch 85 \t train_loss 0.0855 \t val_acc 0.2763 \t macro_f1 0.5035\n",
      "Epoch 86 \t train_loss 0.0855 \t val_acc 0.2764 \t macro_f1 0.5074\n",
      "Epoch 87 \t train_loss 0.0855 \t val_acc 0.2765 \t macro_f1 0.5056\n",
      "Epoch 88 \t train_loss 0.0855 \t val_acc 0.2765 \t macro_f1 0.5043\n",
      "Epoch 89 \t train_loss 0.0855 \t val_acc 0.2765 \t macro_f1 0.5072\n",
      "Early stopping at epoch 89 (no improvement for 10 epochs)\n",
      "======\n",
      "Best epoch 89 \t val_acc 0.2778 \t macro_f1 0.5062\n",
      "CPU times: user 25min 33s, sys: 51min 28s, total: 1h 17min 1s\n",
      "Wall time: 11min 6s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "run_experiment('linear', use_img=False, use_txt=True, use_loc=False, use_mixup=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3c42b1f1-0df5-4c12-b1ec-b05f6b7d378f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00 \t train_loss 0.1444 \t val_acc 0.2052 \t macro_f1 0.3728\n",
      "Epoch 01 \t train_loss 0.1039 \t val_acc 0.2435 \t macro_f1 0.4516\n",
      "Epoch 02 \t train_loss 0.1008 \t val_acc 0.2536 \t macro_f1 0.4753\n",
      "Epoch 03 \t train_loss 0.0994 \t val_acc 0.2610 \t macro_f1 0.4857\n",
      "Epoch 04 \t train_loss 0.0987 \t val_acc 0.2642 \t macro_f1 0.4922\n",
      "Epoch 05 \t train_loss 0.0982 \t val_acc 0.2665 \t macro_f1 0.4958\n",
      "Epoch 06 \t train_loss 0.0978 \t val_acc 0.2689 \t macro_f1 0.5005\n",
      "Epoch 07 \t train_loss 0.0974 \t val_acc 0.2705 \t macro_f1 0.5023\n",
      "Epoch 08 \t train_loss 0.0972 \t val_acc 0.2715 \t macro_f1 0.5047\n",
      "Epoch 09 \t train_loss 0.0970 \t val_acc 0.2720 \t macro_f1 0.5050\n",
      "Epoch 10 \t train_loss 0.0969 \t val_acc 0.2726 \t macro_f1 0.5082\n",
      "Epoch 11 \t train_loss 0.0968 \t val_acc 0.2740 \t macro_f1 0.5090\n",
      "Epoch 12 \t train_loss 0.0967 \t val_acc 0.2744 \t macro_f1 0.5111\n",
      "Epoch 13 \t train_loss 0.0965 \t val_acc 0.2753 \t macro_f1 0.5118\n",
      "Epoch 14 \t train_loss 0.0965 \t val_acc 0.2755 \t macro_f1 0.5119\n",
      "Epoch 15 \t train_loss 0.0964 \t val_acc 0.2760 \t macro_f1 0.5115\n",
      "Epoch 16 \t train_loss 0.0963 \t val_acc 0.2764 \t macro_f1 0.5124\n",
      "Epoch 17 \t train_loss 0.0962 \t val_acc 0.2767 \t macro_f1 0.5141\n",
      "Epoch 18 \t train_loss 0.0962 \t val_acc 0.2776 \t macro_f1 0.5154\n",
      "Epoch 19 \t train_loss 0.0962 \t val_acc 0.2769 \t macro_f1 0.5134\n",
      "Epoch 20 \t train_loss 0.0962 \t val_acc 0.2771 \t macro_f1 0.5129\n",
      "Epoch 21 \t train_loss 0.0961 \t val_acc 0.2777 \t macro_f1 0.5164\n",
      "Epoch 22 \t train_loss 0.0961 \t val_acc 0.2773 \t macro_f1 0.5153\n",
      "Epoch 23 \t train_loss 0.0961 \t val_acc 0.2774 \t macro_f1 0.5155\n",
      "Epoch 24 \t train_loss 0.0961 \t val_acc 0.2774 \t macro_f1 0.5169\n",
      "Epoch 25 \t train_loss 0.0960 \t val_acc 0.2778 \t macro_f1 0.5138\n",
      "Epoch 26 \t train_loss 0.0960 \t val_acc 0.2775 \t macro_f1 0.5163\n",
      "Epoch 27 \t train_loss 0.0960 \t val_acc 0.2780 \t macro_f1 0.5165\n",
      "Epoch 28 \t train_loss 0.0959 \t val_acc 0.2776 \t macro_f1 0.5151\n",
      "Epoch 29 \t train_loss 0.0959 \t val_acc 0.2790 \t macro_f1 0.5184\n",
      "Epoch 30 \t train_loss 0.0959 \t val_acc 0.2779 \t macro_f1 0.5175\n",
      "Epoch 31 \t train_loss 0.0959 \t val_acc 0.2775 \t macro_f1 0.5153\n",
      "Epoch 32 \t train_loss 0.0958 \t val_acc 0.2784 \t macro_f1 0.5171\n",
      "Epoch 33 \t train_loss 0.0959 \t val_acc 0.2784 \t macro_f1 0.5162\n",
      "Epoch 34 \t train_loss 0.0958 \t val_acc 0.2784 \t macro_f1 0.5176\n",
      "Epoch 35 \t train_loss 0.0958 \t val_acc 0.2788 \t macro_f1 0.5176\n",
      "Epoch 36 \t train_loss 0.0958 \t val_acc 0.2790 \t macro_f1 0.5176\n",
      "Epoch 37 \t train_loss 0.0958 \t val_acc 0.2787 \t macro_f1 0.5163\n",
      "Epoch 38 \t train_loss 0.0958 \t val_acc 0.2787 \t macro_f1 0.5168\n",
      "Epoch 39 \t train_loss 0.0957 \t val_acc 0.2789 \t macro_f1 0.5170\n",
      "Epoch 40 \t train_loss 0.0958 \t val_acc 0.2792 \t macro_f1 0.5176\n",
      "Epoch 41 \t train_loss 0.0957 \t val_acc 0.2787 \t macro_f1 0.5173\n",
      "Epoch 42 \t train_loss 0.0958 \t val_acc 0.2787 \t macro_f1 0.5174\n",
      "Epoch 43 \t train_loss 0.0958 \t val_acc 0.2790 \t macro_f1 0.5174\n",
      "Epoch 44 \t train_loss 0.0957 \t val_acc 0.2791 \t macro_f1 0.5172\n",
      "Epoch 45 \t train_loss 0.0957 \t val_acc 0.2789 \t macro_f1 0.5178\n",
      "Epoch 46 \t train_loss 0.0957 \t val_acc 0.2790 \t macro_f1 0.5178\n",
      "Epoch 47 \t train_loss 0.0957 \t val_acc 0.2791 \t macro_f1 0.5177\n",
      "Epoch 48 \t train_loss 0.0957 \t val_acc 0.2791 \t macro_f1 0.5179\n",
      "Epoch 49 \t train_loss 0.0957 \t val_acc 0.2790 \t macro_f1 0.5178\n",
      "Epoch 50 \t train_loss 0.0957 \t val_acc 0.2790 \t macro_f1 0.5178\n",
      "Early stopping at epoch 50 (no improvement for 10 epochs)\n",
      "======\n",
      "Best epoch 50 \t val_acc 0.2792 \t macro_f1 0.5176\n",
      "CPU times: user 20min 32s, sys: 27min 51s, total: 48min 23s\n",
      "Wall time: 6min 33s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "run_experiment('linear', use_img=False, use_txt=True, use_loc=False, use_mixup=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7bd51b2f-4902-47ac-a020-77abfd07a1b8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00 \t train_loss 0.1481 \t val_acc 0.1866 \t macro_f1 0.3106\n",
      "Epoch 01 \t train_loss 0.0989 \t val_acc 0.2285 \t macro_f1 0.3944\n",
      "Epoch 02 \t train_loss 0.0952 \t val_acc 0.2428 \t macro_f1 0.4238\n",
      "Epoch 03 \t train_loss 0.0934 \t val_acc 0.2521 \t macro_f1 0.4391\n",
      "Epoch 04 \t train_loss 0.0923 \t val_acc 0.2560 \t macro_f1 0.4492\n",
      "Epoch 05 \t train_loss 0.0914 \t val_acc 0.2611 \t macro_f1 0.4546\n",
      "Epoch 06 \t train_loss 0.0908 \t val_acc 0.2643 \t macro_f1 0.4586\n",
      "Epoch 07 \t train_loss 0.0903 \t val_acc 0.2691 \t macro_f1 0.4671\n",
      "Epoch 08 \t train_loss 0.0899 \t val_acc 0.2683 \t macro_f1 0.4675\n",
      "Epoch 09 \t train_loss 0.0895 \t val_acc 0.2718 \t macro_f1 0.4729\n",
      "Epoch 10 \t train_loss 0.0892 \t val_acc 0.2727 \t macro_f1 0.4738\n",
      "Epoch 11 \t train_loss 0.0890 \t val_acc 0.2738 \t macro_f1 0.4744\n",
      "Epoch 12 \t train_loss 0.0887 \t val_acc 0.2754 \t macro_f1 0.4783\n",
      "Epoch 13 \t train_loss 0.0885 \t val_acc 0.2744 \t macro_f1 0.4741\n",
      "Epoch 14 \t train_loss 0.0884 \t val_acc 0.2776 \t macro_f1 0.4815\n",
      "Epoch 15 \t train_loss 0.0881 \t val_acc 0.2776 \t macro_f1 0.4820\n",
      "Epoch 16 \t train_loss 0.0880 \t val_acc 0.2800 \t macro_f1 0.4851\n",
      "Epoch 17 \t train_loss 0.0879 \t val_acc 0.2796 \t macro_f1 0.4836\n",
      "Epoch 18 \t train_loss 0.0877 \t val_acc 0.2804 \t macro_f1 0.4824\n",
      "Epoch 19 \t train_loss 0.0876 \t val_acc 0.2804 \t macro_f1 0.4843\n",
      "Epoch 20 \t train_loss 0.0875 \t val_acc 0.2804 \t macro_f1 0.4846\n",
      "Epoch 21 \t train_loss 0.0874 \t val_acc 0.2819 \t macro_f1 0.4899\n",
      "Epoch 22 \t train_loss 0.0872 \t val_acc 0.2801 \t macro_f1 0.4850\n",
      "Epoch 23 \t train_loss 0.0870 \t val_acc 0.2809 \t macro_f1 0.4878\n",
      "Epoch 24 \t train_loss 0.0869 \t val_acc 0.2822 \t macro_f1 0.4890\n",
      "Epoch 25 \t train_loss 0.0869 \t val_acc 0.2834 \t macro_f1 0.4913\n",
      "Epoch 26 \t train_loss 0.0868 \t val_acc 0.2826 \t macro_f1 0.4887\n",
      "Epoch 27 \t train_loss 0.0866 \t val_acc 0.2833 \t macro_f1 0.4900\n",
      "Epoch 28 \t train_loss 0.0865 \t val_acc 0.2838 \t macro_f1 0.4914\n",
      "Epoch 29 \t train_loss 0.0865 \t val_acc 0.2834 \t macro_f1 0.4901\n",
      "Epoch 30 \t train_loss 0.0864 \t val_acc 0.2852 \t macro_f1 0.4927\n",
      "Epoch 31 \t train_loss 0.0863 \t val_acc 0.2856 \t macro_f1 0.4945\n",
      "Epoch 32 \t train_loss 0.0862 \t val_acc 0.2848 \t macro_f1 0.4918\n",
      "Epoch 33 \t train_loss 0.0862 \t val_acc 0.2853 \t macro_f1 0.4925\n",
      "Epoch 34 \t train_loss 0.0861 \t val_acc 0.2849 \t macro_f1 0.4924\n",
      "Epoch 35 \t train_loss 0.0861 \t val_acc 0.2852 \t macro_f1 0.4933\n",
      "Epoch 36 \t train_loss 0.0860 \t val_acc 0.2850 \t macro_f1 0.4927\n",
      "Epoch 37 \t train_loss 0.0860 \t val_acc 0.2856 \t macro_f1 0.4935\n",
      "Epoch 38 \t train_loss 0.0859 \t val_acc 0.2851 \t macro_f1 0.4934\n",
      "Epoch 39 \t train_loss 0.0859 \t val_acc 0.2855 \t macro_f1 0.4945\n",
      "Epoch 40 \t train_loss 0.0858 \t val_acc 0.2853 \t macro_f1 0.4924\n",
      "Epoch 41 \t train_loss 0.0859 \t val_acc 0.2860 \t macro_f1 0.4943\n",
      "Epoch 42 \t train_loss 0.0858 \t val_acc 0.2860 \t macro_f1 0.4938\n",
      "Epoch 43 \t train_loss 0.0858 \t val_acc 0.2854 \t macro_f1 0.4935\n",
      "Epoch 44 \t train_loss 0.0858 \t val_acc 0.2860 \t macro_f1 0.4942\n",
      "Epoch 45 \t train_loss 0.0857 \t val_acc 0.2860 \t macro_f1 0.4939\n",
      "Epoch 46 \t train_loss 0.0857 \t val_acc 0.2860 \t macro_f1 0.4941\n",
      "Epoch 47 \t train_loss 0.0857 \t val_acc 0.2861 \t macro_f1 0.4941\n",
      "Epoch 48 \t train_loss 0.0857 \t val_acc 0.2861 \t macro_f1 0.4942\n",
      "Epoch 49 \t train_loss 0.0857 \t val_acc 0.2861 \t macro_f1 0.4941\n",
      "Epoch 50 \t train_loss 0.0857 \t val_acc 0.2861 \t macro_f1 0.4941\n",
      "Epoch 51 \t train_loss 0.0857 \t val_acc 0.2861 \t macro_f1 0.4941\n",
      "Epoch 52 \t train_loss 0.0857 \t val_acc 0.2861 \t macro_f1 0.4942\n",
      "Epoch 53 \t train_loss 0.0857 \t val_acc 0.2863 \t macro_f1 0.4945\n",
      "Epoch 54 \t train_loss 0.0857 \t val_acc 0.2862 \t macro_f1 0.4942\n",
      "Epoch 55 \t train_loss 0.0858 \t val_acc 0.2859 \t macro_f1 0.4939\n",
      "Epoch 56 \t train_loss 0.0857 \t val_acc 0.2861 \t macro_f1 0.4941\n",
      "Epoch 57 \t train_loss 0.0857 \t val_acc 0.2865 \t macro_f1 0.4945\n",
      "Epoch 58 \t train_loss 0.0857 \t val_acc 0.2861 \t macro_f1 0.4940\n",
      "Epoch 59 \t train_loss 0.0858 \t val_acc 0.2861 \t macro_f1 0.4946\n",
      "Epoch 60 \t train_loss 0.0858 \t val_acc 0.2862 \t macro_f1 0.4937\n",
      "Epoch 61 \t train_loss 0.0858 \t val_acc 0.2858 \t macro_f1 0.4943\n",
      "Epoch 62 \t train_loss 0.0858 \t val_acc 0.2865 \t macro_f1 0.4944\n",
      "Epoch 63 \t train_loss 0.0858 \t val_acc 0.2858 \t macro_f1 0.4932\n",
      "Epoch 64 \t train_loss 0.0858 \t val_acc 0.2864 \t macro_f1 0.4939\n",
      "Epoch 65 \t train_loss 0.0858 \t val_acc 0.2859 \t macro_f1 0.4937\n",
      "Epoch 66 \t train_loss 0.0858 \t val_acc 0.2860 \t macro_f1 0.4932\n",
      "Epoch 67 \t train_loss 0.0858 \t val_acc 0.2849 \t macro_f1 0.4921\n",
      "Epoch 68 \t train_loss 0.0858 \t val_acc 0.2858 \t macro_f1 0.4919\n",
      "Epoch 69 \t train_loss 0.0858 \t val_acc 0.2861 \t macro_f1 0.4948\n",
      "Epoch 70 \t train_loss 0.0858 \t val_acc 0.2853 \t macro_f1 0.4920\n",
      "Epoch 71 \t train_loss 0.0858 \t val_acc 0.2869 \t macro_f1 0.4951\n",
      "Epoch 72 \t train_loss 0.0858 \t val_acc 0.2844 \t macro_f1 0.4912\n",
      "Epoch 73 \t train_loss 0.0858 \t val_acc 0.2859 \t macro_f1 0.4934\n",
      "Epoch 74 \t train_loss 0.0858 \t val_acc 0.2861 \t macro_f1 0.4935\n",
      "Epoch 75 \t train_loss 0.0859 \t val_acc 0.2866 \t macro_f1 0.4946\n",
      "Epoch 76 \t train_loss 0.0858 \t val_acc 0.2852 \t macro_f1 0.4924\n",
      "Epoch 77 \t train_loss 0.0858 \t val_acc 0.2854 \t macro_f1 0.4927\n",
      "Epoch 78 \t train_loss 0.0858 \t val_acc 0.2860 \t macro_f1 0.4921\n",
      "Epoch 79 \t train_loss 0.0858 \t val_acc 0.2854 \t macro_f1 0.4929\n",
      "Epoch 80 \t train_loss 0.0858 \t val_acc 0.2868 \t macro_f1 0.4954\n",
      "Epoch 81 \t train_loss 0.0858 \t val_acc 0.2864 \t macro_f1 0.4920\n",
      "Early stopping at epoch 81 (no improvement for 10 epochs)\n",
      "======\n",
      "Best epoch 81 \t val_acc 0.2869 \t macro_f1 0.4951\n",
      "CPU times: user 25min 24s, sys: 55min 2s, total: 1h 20min 27s\n",
      "Wall time: 11min 35s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "run_experiment('mlp', use_img=False, use_txt=True, use_loc=False, use_mixup=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "590a1f1b-e4ad-4451-8c83-76f8acbd6f80",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00 \t train_loss 0.1526 \t val_acc 0.1726 \t macro_f1 0.2849\n",
      "Epoch 01 \t train_loss 0.1084 \t val_acc 0.2174 \t macro_f1 0.3744\n",
      "Epoch 02 \t train_loss 0.1047 \t val_acc 0.2319 \t macro_f1 0.4040\n",
      "Epoch 03 \t train_loss 0.1031 \t val_acc 0.2391 \t macro_f1 0.4161\n",
      "Epoch 04 \t train_loss 0.1021 \t val_acc 0.2473 \t macro_f1 0.4317\n",
      "Epoch 05 \t train_loss 0.1014 \t val_acc 0.2519 \t macro_f1 0.4366\n",
      "Epoch 06 \t train_loss 0.1009 \t val_acc 0.2564 \t macro_f1 0.4448\n",
      "Epoch 07 \t train_loss 0.1005 \t val_acc 0.2580 \t macro_f1 0.4485\n",
      "Epoch 08 \t train_loss 0.1001 \t val_acc 0.2600 \t macro_f1 0.4508\n",
      "Epoch 09 \t train_loss 0.0998 \t val_acc 0.2630 \t macro_f1 0.4567\n",
      "Epoch 10 \t train_loss 0.0996 \t val_acc 0.2624 \t macro_f1 0.4562\n",
      "Epoch 11 \t train_loss 0.0994 \t val_acc 0.2641 \t macro_f1 0.4575\n",
      "Epoch 12 \t train_loss 0.0991 \t val_acc 0.2657 \t macro_f1 0.4649\n",
      "Epoch 13 \t train_loss 0.0990 \t val_acc 0.2655 \t macro_f1 0.4653\n",
      "Epoch 14 \t train_loss 0.0989 \t val_acc 0.2678 \t macro_f1 0.4653\n",
      "Epoch 15 \t train_loss 0.0988 \t val_acc 0.2694 \t macro_f1 0.4690\n",
      "Epoch 16 \t train_loss 0.0986 \t val_acc 0.2676 \t macro_f1 0.4654\n",
      "Epoch 17 \t train_loss 0.0985 \t val_acc 0.2680 \t macro_f1 0.4653\n",
      "Epoch 18 \t train_loss 0.0984 \t val_acc 0.2710 \t macro_f1 0.4701\n",
      "Epoch 19 \t train_loss 0.0982 \t val_acc 0.2711 \t macro_f1 0.4716\n",
      "Epoch 20 \t train_loss 0.0981 \t val_acc 0.2710 \t macro_f1 0.4707\n",
      "Epoch 21 \t train_loss 0.0980 \t val_acc 0.2732 \t macro_f1 0.4754\n",
      "Epoch 22 \t train_loss 0.0978 \t val_acc 0.2723 \t macro_f1 0.4728\n",
      "Epoch 23 \t train_loss 0.0978 \t val_acc 0.2758 \t macro_f1 0.4776\n",
      "Epoch 24 \t train_loss 0.0977 \t val_acc 0.2742 \t macro_f1 0.4754\n",
      "Epoch 25 \t train_loss 0.0976 \t val_acc 0.2744 \t macro_f1 0.4772\n",
      "Epoch 26 \t train_loss 0.0975 \t val_acc 0.2747 \t macro_f1 0.4798\n",
      "Epoch 27 \t train_loss 0.0975 \t val_acc 0.2737 \t macro_f1 0.4755\n",
      "Epoch 28 \t train_loss 0.0974 \t val_acc 0.2750 \t macro_f1 0.4776\n",
      "Epoch 29 \t train_loss 0.0973 \t val_acc 0.2754 \t macro_f1 0.4772\n",
      "Epoch 30 \t train_loss 0.0972 \t val_acc 0.2762 \t macro_f1 0.4796\n",
      "Epoch 31 \t train_loss 0.0972 \t val_acc 0.2763 \t macro_f1 0.4790\n",
      "Epoch 32 \t train_loss 0.0971 \t val_acc 0.2765 \t macro_f1 0.4806\n",
      "Epoch 33 \t train_loss 0.0970 \t val_acc 0.2775 \t macro_f1 0.4812\n",
      "Epoch 34 \t train_loss 0.0970 \t val_acc 0.2776 \t macro_f1 0.4807\n",
      "Epoch 35 \t train_loss 0.0969 \t val_acc 0.2777 \t macro_f1 0.4814\n",
      "Epoch 36 \t train_loss 0.0969 \t val_acc 0.2782 \t macro_f1 0.4815\n",
      "Epoch 37 \t train_loss 0.0968 \t val_acc 0.2781 \t macro_f1 0.4828\n",
      "Epoch 38 \t train_loss 0.0969 \t val_acc 0.2776 \t macro_f1 0.4819\n",
      "Epoch 39 \t train_loss 0.0967 \t val_acc 0.2781 \t macro_f1 0.4820\n",
      "Epoch 40 \t train_loss 0.0967 \t val_acc 0.2780 \t macro_f1 0.4828\n",
      "Epoch 41 \t train_loss 0.0967 \t val_acc 0.2778 \t macro_f1 0.4817\n",
      "Epoch 42 \t train_loss 0.0966 \t val_acc 0.2787 \t macro_f1 0.4832\n",
      "Epoch 43 \t train_loss 0.0967 \t val_acc 0.2786 \t macro_f1 0.4827\n",
      "Epoch 44 \t train_loss 0.0967 \t val_acc 0.2789 \t macro_f1 0.4832\n",
      "Epoch 45 \t train_loss 0.0966 \t val_acc 0.2791 \t macro_f1 0.4836\n",
      "Epoch 46 \t train_loss 0.0966 \t val_acc 0.2788 \t macro_f1 0.4835\n",
      "Epoch 47 \t train_loss 0.0966 \t val_acc 0.2790 \t macro_f1 0.4835\n",
      "Epoch 48 \t train_loss 0.0966 \t val_acc 0.2790 \t macro_f1 0.4834\n",
      "Epoch 49 \t train_loss 0.0966 \t val_acc 0.2789 \t macro_f1 0.4833\n",
      "Epoch 50 \t train_loss 0.0966 \t val_acc 0.2789 \t macro_f1 0.4833\n",
      "Epoch 51 \t train_loss 0.0966 \t val_acc 0.2789 \t macro_f1 0.4832\n",
      "Epoch 52 \t train_loss 0.0966 \t val_acc 0.2788 \t macro_f1 0.4831\n",
      "Epoch 53 \t train_loss 0.0966 \t val_acc 0.2788 \t macro_f1 0.4830\n",
      "Epoch 54 \t train_loss 0.0966 \t val_acc 0.2787 \t macro_f1 0.4832\n",
      "Epoch 55 \t train_loss 0.0965 \t val_acc 0.2786 \t macro_f1 0.4829\n",
      "Early stopping at epoch 55 (no improvement for 10 epochs)\n",
      "======\n",
      "Best epoch 55 \t val_acc 0.2791 \t macro_f1 0.4836\n",
      "CPU times: user 18min 27s, sys: 40min 16s, total: 58min 43s\n",
      "Wall time: 8min 26s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "run_experiment('mlp', use_img=False, use_txt=True, use_loc=False, use_mixup=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dd6a7ec-c4eb-4562-b96b-50efc213c66e",
   "metadata": {},
   "source": [
    "## Location only\n",
    "* Here, validation accuracy never exceeds 0.0 so we don't even save the best model/head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "97f0cc40-bb2b-4dc2-b84f-01aee6ba063a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00 \t train_loss 0.6381 \t val_acc 0.0000 \t macro_f1 0.0044\n",
      "Epoch 01 \t train_loss 0.4418 \t val_acc 0.0000 \t macro_f1 0.0000\n",
      "Epoch 02 \t train_loss 0.3290 \t val_acc 0.0000 \t macro_f1 0.0000\n",
      "Epoch 03 \t train_loss 0.2643 \t val_acc 0.0000 \t macro_f1 0.0000\n",
      "Epoch 04 \t train_loss 0.2262 \t val_acc 0.0000 \t macro_f1 0.0000\n",
      "Epoch 05 \t train_loss 0.2027 \t val_acc 0.0000 \t macro_f1 0.0000\n",
      "Epoch 06 \t train_loss 0.1879 \t val_acc 0.0000 \t macro_f1 0.0000\n",
      "Epoch 07 \t train_loss 0.1782 \t val_acc 0.0000 \t macro_f1 0.0000\n",
      "Epoch 08 \t train_loss 0.1718 \t val_acc 0.0000 \t macro_f1 0.0000\n",
      "Epoch 09 \t train_loss 0.1673 \t val_acc 0.0000 \t macro_f1 0.0000\n",
      "Early stopping at epoch 9 (no improvement for 10 epochs)\n",
      "======\n",
      "Best epoch 09 \t val_acc 0.0000 \t macro_f1 0.0000\n",
      "CPU times: user 3min 26s, sys: 5min 3s, total: 8min 30s\n",
      "Wall time: 1min 11s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "run_experiment('linear', use_img=False, use_txt=False, use_loc=True, use_mixup=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "646b5c49-5534-4ccc-9640-d7e4b8e3500e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00 \t train_loss 0.6318 \t val_acc 0.0000 \t macro_f1 0.0076\n",
      "Epoch 01 \t train_loss 0.4380 \t val_acc 0.0000 \t macro_f1 0.0000\n",
      "Epoch 02 \t train_loss 0.3268 \t val_acc 0.0000 \t macro_f1 0.0000\n",
      "Epoch 03 \t train_loss 0.2629 \t val_acc 0.0000 \t macro_f1 0.0000\n",
      "Epoch 04 \t train_loss 0.2252 \t val_acc 0.0000 \t macro_f1 0.0000\n",
      "Epoch 05 \t train_loss 0.2021 \t val_acc 0.0000 \t macro_f1 0.0000\n",
      "Epoch 06 \t train_loss 0.1874 \t val_acc 0.0000 \t macro_f1 0.0000\n",
      "Epoch 07 \t train_loss 0.1779 \t val_acc 0.0000 \t macro_f1 0.0000\n",
      "Epoch 08 \t train_loss 0.1714 \t val_acc 0.0000 \t macro_f1 0.0000\n",
      "Epoch 09 \t train_loss 0.1670 \t val_acc 0.0000 \t macro_f1 0.0000\n",
      "Early stopping at epoch 9 (no improvement for 10 epochs)\n",
      "======\n",
      "Best epoch 09 \t val_acc 0.0000 \t macro_f1 0.0000\n",
      "CPU times: user 3min 47s, sys: 4min 49s, total: 8min 36s\n",
      "Wall time: 1min 7s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "run_experiment('linear', use_img=False, use_txt=False, use_loc=True, use_mixup=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "17a61e11-9e19-45b5-af05-a7f427fcb0bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00 \t train_loss 0.1885 \t val_acc 0.0000 \t macro_f1 0.0000\n",
      "Epoch 01 \t train_loss 0.1578 \t val_acc 0.0000 \t macro_f1 0.0000\n",
      "Epoch 02 \t train_loss 0.1576 \t val_acc 0.0000 \t macro_f1 0.0000\n",
      "Epoch 03 \t train_loss 0.1575 \t val_acc 0.0000 \t macro_f1 0.0000\n",
      "Epoch 04 \t train_loss 0.1574 \t val_acc 0.0000 \t macro_f1 0.0000\n",
      "Epoch 05 \t train_loss 0.1573 \t val_acc 0.0000 \t macro_f1 0.0000\n",
      "Epoch 06 \t train_loss 0.1572 \t val_acc 0.0000 \t macro_f1 0.0000\n",
      "Epoch 07 \t train_loss 0.1572 \t val_acc 0.0000 \t macro_f1 0.0000\n",
      "Epoch 08 \t train_loss 0.1571 \t val_acc 0.0000 \t macro_f1 0.0000\n",
      "Epoch 09 \t train_loss 0.1571 \t val_acc 0.0000 \t macro_f1 0.0000\n",
      "Early stopping at epoch 9 (no improvement for 10 epochs)\n",
      "======\n",
      "Best epoch 09 \t val_acc 0.0000 \t macro_f1 0.0000\n",
      "CPU times: user 4min 2s, sys: 5min 29s, total: 9min 32s\n",
      "Wall time: 1min 18s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "run_experiment('mlp', use_img=False, use_txt=False, use_loc=True, use_mixup=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c190ba8f-c706-4db4-8ce5-02b305204b3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00 \t train_loss 0.1884 \t val_acc 0.0000 \t macro_f1 0.0000\n",
      "Epoch 01 \t train_loss 0.1579 \t val_acc 0.0000 \t macro_f1 0.0000\n",
      "Epoch 02 \t train_loss 0.1577 \t val_acc 0.0000 \t macro_f1 0.0000\n",
      "Epoch 03 \t train_loss 0.1575 \t val_acc 0.0000 \t macro_f1 0.0000\n",
      "Epoch 04 \t train_loss 0.1574 \t val_acc 0.0000 \t macro_f1 0.0000\n",
      "Epoch 05 \t train_loss 0.1573 \t val_acc 0.0000 \t macro_f1 0.0000\n",
      "Epoch 06 \t train_loss 0.1572 \t val_acc 0.0000 \t macro_f1 0.0000\n",
      "Epoch 07 \t train_loss 0.1572 \t val_acc 0.0000 \t macro_f1 0.0000\n",
      "Epoch 08 \t train_loss 0.1572 \t val_acc 0.0000 \t macro_f1 0.0000\n",
      "Epoch 09 \t train_loss 0.1572 \t val_acc 0.0000 \t macro_f1 0.0000\n",
      "Early stopping at epoch 9 (no improvement for 10 epochs)\n",
      "======\n",
      "Best epoch 09 \t val_acc 0.0000 \t macro_f1 0.0000\n",
      "CPU times: user 3min 56s, sys: 5min 33s, total: 9min 29s\n",
      "Wall time: 1min 17s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "run_experiment('mlp', use_img=False, use_txt=False, use_loc=True, use_mixup=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba62188b-62dc-4f68-a80c-20e757604a82",
   "metadata": {},
   "source": [
    "### Image + title embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4c9e0bc4-1fc5-4c33-81c8-2ad651ca3703",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00 \t train_loss 0.1192 \t val_acc 0.2681 \t macro_f1 0.4954\n",
      "Epoch 01 \t train_loss 0.0897 \t val_acc 0.2923 \t macro_f1 0.5411\n",
      "Epoch 02 \t train_loss 0.0874 \t val_acc 0.3026 \t macro_f1 0.5566\n",
      "Epoch 03 \t train_loss 0.0864 \t val_acc 0.3082 \t macro_f1 0.5673\n",
      "Epoch 04 \t train_loss 0.0857 \t val_acc 0.3122 \t macro_f1 0.5740\n",
      "Epoch 05 \t train_loss 0.0853 \t val_acc 0.3146 \t macro_f1 0.5777\n",
      "Epoch 06 \t train_loss 0.0850 \t val_acc 0.3171 \t macro_f1 0.5782\n",
      "Epoch 07 \t train_loss 0.0847 \t val_acc 0.3181 \t macro_f1 0.5821\n",
      "Epoch 08 \t train_loss 0.0845 \t val_acc 0.3207 \t macro_f1 0.5849\n",
      "Epoch 09 \t train_loss 0.0843 \t val_acc 0.3209 \t macro_f1 0.5862\n",
      "Epoch 10 \t train_loss 0.0843 \t val_acc 0.3208 \t macro_f1 0.5881\n",
      "Epoch 11 \t train_loss 0.0841 \t val_acc 0.3225 \t macro_f1 0.5884\n",
      "Epoch 12 \t train_loss 0.0840 \t val_acc 0.3235 \t macro_f1 0.5911\n",
      "Epoch 13 \t train_loss 0.0839 \t val_acc 0.3225 \t macro_f1 0.5895\n",
      "Epoch 14 \t train_loss 0.0839 \t val_acc 0.3238 \t macro_f1 0.5913\n",
      "Epoch 15 \t train_loss 0.0838 \t val_acc 0.3238 \t macro_f1 0.5934\n",
      "Epoch 16 \t train_loss 0.0837 \t val_acc 0.3246 \t macro_f1 0.5927\n",
      "Epoch 17 \t train_loss 0.0837 \t val_acc 0.3245 \t macro_f1 0.5919\n",
      "Epoch 18 \t train_loss 0.0836 \t val_acc 0.3243 \t macro_f1 0.5916\n",
      "Epoch 19 \t train_loss 0.0836 \t val_acc 0.3247 \t macro_f1 0.5916\n",
      "Epoch 20 \t train_loss 0.0836 \t val_acc 0.3257 \t macro_f1 0.5954\n",
      "Epoch 21 \t train_loss 0.0836 \t val_acc 0.3260 \t macro_f1 0.5924\n",
      "Epoch 22 \t train_loss 0.0835 \t val_acc 0.3260 \t macro_f1 0.5932\n",
      "Epoch 23 \t train_loss 0.0834 \t val_acc 0.3265 \t macro_f1 0.5965\n",
      "Epoch 24 \t train_loss 0.0834 \t val_acc 0.3265 \t macro_f1 0.5956\n",
      "Epoch 25 \t train_loss 0.0834 \t val_acc 0.3262 \t macro_f1 0.5955\n",
      "Epoch 26 \t train_loss 0.0834 \t val_acc 0.3264 \t macro_f1 0.5947\n",
      "Epoch 27 \t train_loss 0.0834 \t val_acc 0.3263 \t macro_f1 0.5950\n",
      "Epoch 28 \t train_loss 0.0833 \t val_acc 0.3264 \t macro_f1 0.5969\n",
      "Epoch 29 \t train_loss 0.0833 \t val_acc 0.3264 \t macro_f1 0.5951\n",
      "Epoch 30 \t train_loss 0.0832 \t val_acc 0.3261 \t macro_f1 0.5956\n",
      "Epoch 31 \t train_loss 0.0832 \t val_acc 0.3274 \t macro_f1 0.5949\n",
      "Epoch 32 \t train_loss 0.0832 \t val_acc 0.3270 \t macro_f1 0.5957\n",
      "Epoch 33 \t train_loss 0.0832 \t val_acc 0.3268 \t macro_f1 0.5942\n",
      "Epoch 34 \t train_loss 0.0831 \t val_acc 0.3270 \t macro_f1 0.5961\n",
      "Epoch 35 \t train_loss 0.0831 \t val_acc 0.3265 \t macro_f1 0.5971\n",
      "Epoch 36 \t train_loss 0.0831 \t val_acc 0.3274 \t macro_f1 0.5951\n",
      "Epoch 37 \t train_loss 0.0831 \t val_acc 0.3271 \t macro_f1 0.5960\n",
      "Epoch 38 \t train_loss 0.0831 \t val_acc 0.3272 \t macro_f1 0.5960\n",
      "Epoch 39 \t train_loss 0.0830 \t val_acc 0.3270 \t macro_f1 0.5958\n",
      "Epoch 40 \t train_loss 0.0830 \t val_acc 0.3274 \t macro_f1 0.5974\n",
      "Epoch 41 \t train_loss 0.0830 \t val_acc 0.3269 \t macro_f1 0.5966\n",
      "Early stopping at epoch 41 (no improvement for 10 epochs)\n",
      "======\n",
      "Best epoch 41 \t val_acc 0.3274 \t macro_f1 0.5949\n",
      "CPU times: user 13min 22s, sys: 27min 43s, total: 41min 6s\n",
      "Wall time: 5min 55s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "run_experiment('linear', use_img=True, use_txt=True, use_loc=False, use_mixup=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5acb3f8f-793c-4c91-a160-8473a40eae5d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00 \t train_loss 0.1091 \t val_acc 0.2667 \t macro_f1 0.4848\n",
      "Epoch 01 \t train_loss 0.0778 \t val_acc 0.2912 \t macro_f1 0.5299\n",
      "Epoch 02 \t train_loss 0.0752 \t val_acc 0.3015 \t macro_f1 0.5442\n",
      "Epoch 03 \t train_loss 0.0739 \t val_acc 0.3070 \t macro_f1 0.5557\n",
      "Epoch 04 \t train_loss 0.0731 \t val_acc 0.3121 \t macro_f1 0.5613\n",
      "Epoch 05 \t train_loss 0.0726 \t val_acc 0.3134 \t macro_f1 0.5642\n",
      "Epoch 06 \t train_loss 0.0722 \t val_acc 0.3164 \t macro_f1 0.5679\n",
      "Epoch 07 \t train_loss 0.0719 \t val_acc 0.3173 \t macro_f1 0.5695\n",
      "Epoch 08 \t train_loss 0.0716 \t val_acc 0.3194 \t macro_f1 0.5733\n",
      "Epoch 09 \t train_loss 0.0714 \t val_acc 0.3196 \t macro_f1 0.5707\n",
      "Epoch 10 \t train_loss 0.0713 \t val_acc 0.3203 \t macro_f1 0.5739\n",
      "Epoch 11 \t train_loss 0.0711 \t val_acc 0.3220 \t macro_f1 0.5772\n",
      "Epoch 12 \t train_loss 0.0710 \t val_acc 0.3214 \t macro_f1 0.5754\n",
      "Epoch 13 \t train_loss 0.0709 \t val_acc 0.3222 \t macro_f1 0.5744\n",
      "Epoch 14 \t train_loss 0.0708 \t val_acc 0.3228 \t macro_f1 0.5785\n",
      "Epoch 15 \t train_loss 0.0707 \t val_acc 0.3237 \t macro_f1 0.5789\n",
      "Epoch 16 \t train_loss 0.0707 \t val_acc 0.3227 \t macro_f1 0.5805\n",
      "Epoch 17 \t train_loss 0.0706 \t val_acc 0.3239 \t macro_f1 0.5779\n",
      "Epoch 18 \t train_loss 0.0705 \t val_acc 0.3238 \t macro_f1 0.5791\n",
      "Epoch 19 \t train_loss 0.0705 \t val_acc 0.3246 \t macro_f1 0.5797\n",
      "Epoch 20 \t train_loss 0.0704 \t val_acc 0.3260 \t macro_f1 0.5839\n",
      "Epoch 21 \t train_loss 0.0704 \t val_acc 0.3246 \t macro_f1 0.5805\n",
      "Epoch 22 \t train_loss 0.0703 \t val_acc 0.3257 \t macro_f1 0.5829\n",
      "Epoch 23 \t train_loss 0.0703 \t val_acc 0.3255 \t macro_f1 0.5828\n",
      "Epoch 24 \t train_loss 0.0702 \t val_acc 0.3255 \t macro_f1 0.5809\n",
      "Epoch 25 \t train_loss 0.0702 \t val_acc 0.3254 \t macro_f1 0.5814\n",
      "Epoch 26 \t train_loss 0.0702 \t val_acc 0.3268 \t macro_f1 0.5837\n",
      "Epoch 27 \t train_loss 0.0701 \t val_acc 0.3259 \t macro_f1 0.5840\n",
      "Epoch 28 \t train_loss 0.0701 \t val_acc 0.3258 \t macro_f1 0.5821\n",
      "Epoch 29 \t train_loss 0.0701 \t val_acc 0.3254 \t macro_f1 0.5824\n",
      "Epoch 30 \t train_loss 0.0700 \t val_acc 0.3262 \t macro_f1 0.5841\n",
      "Epoch 31 \t train_loss 0.0700 \t val_acc 0.3259 \t macro_f1 0.5823\n",
      "Epoch 32 \t train_loss 0.0700 \t val_acc 0.3270 \t macro_f1 0.5850\n",
      "Epoch 33 \t train_loss 0.0699 \t val_acc 0.3264 \t macro_f1 0.5855\n",
      "Epoch 34 \t train_loss 0.0699 \t val_acc 0.3267 \t macro_f1 0.5856\n",
      "Epoch 35 \t train_loss 0.0699 \t val_acc 0.3259 \t macro_f1 0.5834\n",
      "Epoch 36 \t train_loss 0.0698 \t val_acc 0.3259 \t macro_f1 0.5844\n",
      "Epoch 37 \t train_loss 0.0698 \t val_acc 0.3266 \t macro_f1 0.5837\n",
      "Epoch 38 \t train_loss 0.0698 \t val_acc 0.3264 \t macro_f1 0.5848\n",
      "Epoch 39 \t train_loss 0.0698 \t val_acc 0.3265 \t macro_f1 0.5844\n",
      "Epoch 40 \t train_loss 0.0698 \t val_acc 0.3269 \t macro_f1 0.5855\n",
      "Epoch 41 \t train_loss 0.0697 \t val_acc 0.3256 \t macro_f1 0.5835\n",
      "Epoch 42 \t train_loss 0.0697 \t val_acc 0.3265 \t macro_f1 0.5850\n",
      "Early stopping at epoch 42 (no improvement for 10 epochs)\n",
      "======\n",
      "Best epoch 42 \t val_acc 0.3270 \t macro_f1 0.5850\n",
      "CPU times: user 17min 21s, sys: 22min 6s, total: 39min 28s\n",
      "Wall time: 5min 19s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "run_experiment('linear', use_img=True, use_txt=True, use_loc=False, use_mixup=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2026c62d-ee53-4df7-a1ba-bdfd55dd5f01",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00 \t train_loss 0.1411 \t val_acc 0.1881 \t macro_f1 0.2901\n",
      "Epoch 01 \t train_loss 0.1010 \t val_acc 0.2414 \t macro_f1 0.3976\n",
      "Epoch 02 \t train_loss 0.0969 \t val_acc 0.2625 \t macro_f1 0.4407\n",
      "Epoch 03 \t train_loss 0.0950 \t val_acc 0.2743 \t macro_f1 0.4630\n",
      "Epoch 04 \t train_loss 0.0939 \t val_acc 0.2787 \t macro_f1 0.4699\n",
      "Epoch 05 \t train_loss 0.0931 \t val_acc 0.2839 \t macro_f1 0.4770\n",
      "Epoch 06 \t train_loss 0.0925 \t val_acc 0.2866 \t macro_f1 0.4819\n",
      "Epoch 07 \t train_loss 0.0920 \t val_acc 0.2919 \t macro_f1 0.4926\n",
      "Epoch 08 \t train_loss 0.0918 \t val_acc 0.2910 \t macro_f1 0.4897\n",
      "Epoch 09 \t train_loss 0.0915 \t val_acc 0.2939 \t macro_f1 0.4939\n",
      "Epoch 10 \t train_loss 0.0912 \t val_acc 0.2980 \t macro_f1 0.4992\n",
      "Epoch 11 \t train_loss 0.0911 \t val_acc 0.2994 \t macro_f1 0.5041\n",
      "Epoch 12 \t train_loss 0.0908 \t val_acc 0.3002 \t macro_f1 0.5030\n",
      "Epoch 13 \t train_loss 0.0906 \t val_acc 0.3007 \t macro_f1 0.5048\n",
      "Epoch 14 \t train_loss 0.0904 \t val_acc 0.3000 \t macro_f1 0.5015\n",
      "Epoch 15 \t train_loss 0.0902 \t val_acc 0.3043 \t macro_f1 0.5087\n",
      "Epoch 16 \t train_loss 0.0901 \t val_acc 0.3023 \t macro_f1 0.5076\n",
      "Epoch 17 \t train_loss 0.0900 \t val_acc 0.3043 \t macro_f1 0.5088\n",
      "Epoch 18 \t train_loss 0.0898 \t val_acc 0.3028 \t macro_f1 0.5052\n",
      "Epoch 19 \t train_loss 0.0896 \t val_acc 0.3058 \t macro_f1 0.5134\n",
      "Epoch 20 \t train_loss 0.0894 \t val_acc 0.3079 \t macro_f1 0.5146\n",
      "Epoch 21 \t train_loss 0.0893 \t val_acc 0.3078 \t macro_f1 0.5165\n",
      "Epoch 22 \t train_loss 0.0892 \t val_acc 0.3060 \t macro_f1 0.5127\n",
      "Epoch 23 \t train_loss 0.0891 \t val_acc 0.3098 \t macro_f1 0.5174\n",
      "Epoch 24 \t train_loss 0.0891 \t val_acc 0.3076 \t macro_f1 0.5144\n",
      "Epoch 25 \t train_loss 0.0891 \t val_acc 0.3098 \t macro_f1 0.5182\n",
      "Epoch 26 \t train_loss 0.0889 \t val_acc 0.3077 \t macro_f1 0.5150\n",
      "Epoch 27 \t train_loss 0.0889 \t val_acc 0.3105 \t macro_f1 0.5194\n",
      "Epoch 28 \t train_loss 0.0888 \t val_acc 0.3105 \t macro_f1 0.5177\n",
      "Epoch 29 \t train_loss 0.0887 \t val_acc 0.3093 \t macro_f1 0.5193\n",
      "Epoch 30 \t train_loss 0.0886 \t val_acc 0.3107 \t macro_f1 0.5183\n",
      "Epoch 31 \t train_loss 0.0885 \t val_acc 0.3116 \t macro_f1 0.5192\n",
      "Epoch 32 \t train_loss 0.0884 \t val_acc 0.3110 \t macro_f1 0.5207\n",
      "Epoch 33 \t train_loss 0.0884 \t val_acc 0.3120 \t macro_f1 0.5205\n",
      "Epoch 34 \t train_loss 0.0883 \t val_acc 0.3114 \t macro_f1 0.5217\n",
      "Epoch 35 \t train_loss 0.0883 \t val_acc 0.3114 \t macro_f1 0.5212\n",
      "Epoch 36 \t train_loss 0.0882 \t val_acc 0.3120 \t macro_f1 0.5215\n",
      "Epoch 37 \t train_loss 0.0882 \t val_acc 0.3116 \t macro_f1 0.5205\n",
      "Epoch 38 \t train_loss 0.0881 \t val_acc 0.3117 \t macro_f1 0.5201\n",
      "Epoch 39 \t train_loss 0.0881 \t val_acc 0.3126 \t macro_f1 0.5217\n",
      "Epoch 40 \t train_loss 0.0880 \t val_acc 0.3122 \t macro_f1 0.5207\n",
      "Epoch 41 \t train_loss 0.0881 \t val_acc 0.3124 \t macro_f1 0.5212\n",
      "Epoch 42 \t train_loss 0.0880 \t val_acc 0.3124 \t macro_f1 0.5217\n",
      "Epoch 43 \t train_loss 0.0879 \t val_acc 0.3127 \t macro_f1 0.5224\n",
      "Epoch 44 \t train_loss 0.0880 \t val_acc 0.3127 \t macro_f1 0.5219\n",
      "Epoch 45 \t train_loss 0.0879 \t val_acc 0.3126 \t macro_f1 0.5225\n",
      "Epoch 46 \t train_loss 0.0880 \t val_acc 0.3128 \t macro_f1 0.5225\n",
      "Epoch 47 \t train_loss 0.0879 \t val_acc 0.3129 \t macro_f1 0.5223\n",
      "Epoch 48 \t train_loss 0.0879 \t val_acc 0.3130 \t macro_f1 0.5225\n",
      "Epoch 49 \t train_loss 0.0879 \t val_acc 0.3129 \t macro_f1 0.5225\n",
      "Epoch 50 \t train_loss 0.0879 \t val_acc 0.3129 \t macro_f1 0.5225\n",
      "Epoch 51 \t train_loss 0.0880 \t val_acc 0.3129 \t macro_f1 0.5225\n",
      "Epoch 52 \t train_loss 0.0879 \t val_acc 0.3129 \t macro_f1 0.5225\n",
      "Epoch 53 \t train_loss 0.0879 \t val_acc 0.3129 \t macro_f1 0.5225\n",
      "Epoch 54 \t train_loss 0.0879 \t val_acc 0.3130 \t macro_f1 0.5225\n",
      "Epoch 55 \t train_loss 0.0880 \t val_acc 0.3132 \t macro_f1 0.5231\n",
      "Epoch 56 \t train_loss 0.0880 \t val_acc 0.3127 \t macro_f1 0.5224\n",
      "Epoch 57 \t train_loss 0.0880 \t val_acc 0.3129 \t macro_f1 0.5221\n",
      "Epoch 58 \t train_loss 0.0879 \t val_acc 0.3126 \t macro_f1 0.5225\n",
      "Epoch 59 \t train_loss 0.0880 \t val_acc 0.3126 \t macro_f1 0.5222\n",
      "Epoch 60 \t train_loss 0.0879 \t val_acc 0.3131 \t macro_f1 0.5229\n",
      "Epoch 61 \t train_loss 0.0880 \t val_acc 0.3131 \t macro_f1 0.5218\n",
      "Epoch 62 \t train_loss 0.0879 \t val_acc 0.3128 \t macro_f1 0.5224\n",
      "Epoch 63 \t train_loss 0.0879 \t val_acc 0.3128 \t macro_f1 0.5252\n",
      "Epoch 64 \t train_loss 0.0880 \t val_acc 0.3121 \t macro_f1 0.5203\n",
      "Epoch 65 \t train_loss 0.0880 \t val_acc 0.3132 \t macro_f1 0.5220\n",
      "Epoch 66 \t train_loss 0.0879 \t val_acc 0.3129 \t macro_f1 0.5237\n",
      "Epoch 67 \t train_loss 0.0879 \t val_acc 0.3131 \t macro_f1 0.5225\n",
      "Epoch 68 \t train_loss 0.0880 \t val_acc 0.3134 \t macro_f1 0.5238\n",
      "Epoch 69 \t train_loss 0.0879 \t val_acc 0.3133 \t macro_f1 0.5236\n",
      "Epoch 70 \t train_loss 0.0879 \t val_acc 0.3142 \t macro_f1 0.5253\n",
      "Epoch 71 \t train_loss 0.0879 \t val_acc 0.3139 \t macro_f1 0.5245\n",
      "Epoch 72 \t train_loss 0.0880 \t val_acc 0.3120 \t macro_f1 0.5217\n",
      "Epoch 73 \t train_loss 0.0880 \t val_acc 0.3118 \t macro_f1 0.5226\n",
      "Epoch 74 \t train_loss 0.0879 \t val_acc 0.3129 \t macro_f1 0.5225\n",
      "Epoch 75 \t train_loss 0.0879 \t val_acc 0.3129 \t macro_f1 0.5245\n",
      "Epoch 76 \t train_loss 0.0880 \t val_acc 0.3137 \t macro_f1 0.5250\n",
      "Epoch 77 \t train_loss 0.0880 \t val_acc 0.3136 \t macro_f1 0.5230\n",
      "Epoch 78 \t train_loss 0.0880 \t val_acc 0.3144 \t macro_f1 0.5282\n",
      "Epoch 79 \t train_loss 0.0879 \t val_acc 0.3129 \t macro_f1 0.5264\n",
      "Epoch 80 \t train_loss 0.0879 \t val_acc 0.3139 \t macro_f1 0.5281\n",
      "Epoch 81 \t train_loss 0.0879 \t val_acc 0.3137 \t macro_f1 0.5260\n",
      "Epoch 82 \t train_loss 0.0880 \t val_acc 0.3148 \t macro_f1 0.5260\n",
      "Epoch 83 \t train_loss 0.0880 \t val_acc 0.3128 \t macro_f1 0.5256\n",
      "Epoch 84 \t train_loss 0.0879 \t val_acc 0.3131 \t macro_f1 0.5244\n",
      "Epoch 85 \t train_loss 0.0879 \t val_acc 0.3128 \t macro_f1 0.5261\n",
      "Epoch 86 \t train_loss 0.0880 \t val_acc 0.3121 \t macro_f1 0.5246\n",
      "Epoch 87 \t train_loss 0.0880 \t val_acc 0.3144 \t macro_f1 0.5295\n",
      "Epoch 88 \t train_loss 0.0881 \t val_acc 0.3147 \t macro_f1 0.5298\n",
      "Epoch 89 \t train_loss 0.0880 \t val_acc 0.3138 \t macro_f1 0.5271\n",
      "Epoch 90 \t train_loss 0.0879 \t val_acc 0.3138 \t macro_f1 0.5293\n",
      "Epoch 91 \t train_loss 0.0879 \t val_acc 0.3126 \t macro_f1 0.5278\n",
      "Epoch 92 \t train_loss 0.0880 \t val_acc 0.3142 \t macro_f1 0.5318\n",
      "Early stopping at epoch 92 (no improvement for 10 epochs)\n",
      "======\n",
      "Best epoch 92 \t val_acc 0.3148 \t macro_f1 0.5260\n",
      "CPU times: user 33min 50s, sys: 1h 11min 14s, total: 1h 45min 4s\n",
      "Wall time: 15min 9s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "run_experiment('mlp', use_img=True, use_txt=True, use_loc=False, use_mixup=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d102a086-cde7-4a99-9909-043b74d8b5a6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00 \t train_loss 0.1306 \t val_acc 0.2103 \t macro_f1 0.3338\n",
      "Epoch 01 \t train_loss 0.0887 \t val_acc 0.2589 \t macro_f1 0.4318\n",
      "Epoch 02 \t train_loss 0.0845 \t val_acc 0.2790 \t macro_f1 0.4707\n",
      "Epoch 03 \t train_loss 0.0825 \t val_acc 0.2865 \t macro_f1 0.4859\n",
      "Epoch 04 \t train_loss 0.0813 \t val_acc 0.2957 \t macro_f1 0.4984\n",
      "Epoch 05 \t train_loss 0.0804 \t val_acc 0.2989 \t macro_f1 0.5048\n",
      "Epoch 06 \t train_loss 0.0798 \t val_acc 0.3062 \t macro_f1 0.5164\n",
      "Epoch 07 \t train_loss 0.0793 \t val_acc 0.3047 \t macro_f1 0.5141\n",
      "Epoch 08 \t train_loss 0.0790 \t val_acc 0.3095 \t macro_f1 0.5197\n",
      "Epoch 09 \t train_loss 0.0786 \t val_acc 0.3114 \t macro_f1 0.5235\n",
      "Epoch 10 \t train_loss 0.0784 \t val_acc 0.3126 \t macro_f1 0.5268\n",
      "Epoch 11 \t train_loss 0.0781 \t val_acc 0.3152 \t macro_f1 0.5276\n",
      "Epoch 12 \t train_loss 0.0779 \t val_acc 0.3149 \t macro_f1 0.5277\n",
      "Epoch 13 \t train_loss 0.0778 \t val_acc 0.3173 \t macro_f1 0.5289\n",
      "Epoch 14 \t train_loss 0.0776 \t val_acc 0.3151 \t macro_f1 0.5274\n",
      "Epoch 15 \t train_loss 0.0774 \t val_acc 0.3187 \t macro_f1 0.5326\n",
      "Epoch 16 \t train_loss 0.0772 \t val_acc 0.3185 \t macro_f1 0.5343\n",
      "Epoch 17 \t train_loss 0.0771 \t val_acc 0.3183 \t macro_f1 0.5327\n",
      "Epoch 18 \t train_loss 0.0770 \t val_acc 0.3174 \t macro_f1 0.5335\n",
      "Epoch 19 \t train_loss 0.0769 \t val_acc 0.3187 \t macro_f1 0.5353\n",
      "Epoch 20 \t train_loss 0.0767 \t val_acc 0.3183 \t macro_f1 0.5328\n",
      "Epoch 21 \t train_loss 0.0765 \t val_acc 0.3213 \t macro_f1 0.5396\n",
      "Epoch 22 \t train_loss 0.0765 \t val_acc 0.3212 \t macro_f1 0.5380\n",
      "Epoch 23 \t train_loss 0.0764 \t val_acc 0.3201 \t macro_f1 0.5349\n",
      "Epoch 24 \t train_loss 0.0763 \t val_acc 0.3225 \t macro_f1 0.5391\n",
      "Epoch 25 \t train_loss 0.0762 \t val_acc 0.3209 \t macro_f1 0.5345\n",
      "Epoch 26 \t train_loss 0.0761 \t val_acc 0.3219 \t macro_f1 0.5373\n",
      "Epoch 27 \t train_loss 0.0760 \t val_acc 0.3228 \t macro_f1 0.5415\n",
      "Epoch 28 \t train_loss 0.0759 \t val_acc 0.3225 \t macro_f1 0.5360\n",
      "Epoch 29 \t train_loss 0.0759 \t val_acc 0.3225 \t macro_f1 0.5378\n",
      "Epoch 30 \t train_loss 0.0758 \t val_acc 0.3219 \t macro_f1 0.5383\n",
      "Epoch 31 \t train_loss 0.0758 \t val_acc 0.3227 \t macro_f1 0.5409\n",
      "Epoch 32 \t train_loss 0.0757 \t val_acc 0.3229 \t macro_f1 0.5383\n",
      "Epoch 33 \t train_loss 0.0756 \t val_acc 0.3235 \t macro_f1 0.5411\n",
      "Epoch 34 \t train_loss 0.0755 \t val_acc 0.3234 \t macro_f1 0.5405\n",
      "Epoch 35 \t train_loss 0.0755 \t val_acc 0.3227 \t macro_f1 0.5378\n",
      "Epoch 36 \t train_loss 0.0754 \t val_acc 0.3231 \t macro_f1 0.5393\n",
      "Epoch 37 \t train_loss 0.0754 \t val_acc 0.3230 \t macro_f1 0.5380\n",
      "Epoch 38 \t train_loss 0.0754 \t val_acc 0.3230 \t macro_f1 0.5383\n",
      "Epoch 39 \t train_loss 0.0753 \t val_acc 0.3243 \t macro_f1 0.5404\n",
      "Epoch 40 \t train_loss 0.0753 \t val_acc 0.3237 \t macro_f1 0.5402\n",
      "Epoch 41 \t train_loss 0.0753 \t val_acc 0.3234 \t macro_f1 0.5396\n",
      "Epoch 42 \t train_loss 0.0753 \t val_acc 0.3236 \t macro_f1 0.5405\n",
      "Epoch 43 \t train_loss 0.0752 \t val_acc 0.3238 \t macro_f1 0.5402\n",
      "Epoch 44 \t train_loss 0.0752 \t val_acc 0.3236 \t macro_f1 0.5398\n",
      "Epoch 45 \t train_loss 0.0751 \t val_acc 0.3237 \t macro_f1 0.5402\n",
      "Epoch 46 \t train_loss 0.0751 \t val_acc 0.3237 \t macro_f1 0.5405\n",
      "Epoch 47 \t train_loss 0.0752 \t val_acc 0.3236 \t macro_f1 0.5403\n",
      "Epoch 48 \t train_loss 0.0751 \t val_acc 0.3237 \t macro_f1 0.5405\n",
      "Epoch 49 \t train_loss 0.0752 \t val_acc 0.3236 \t macro_f1 0.5404\n",
      "Early stopping at epoch 49 (no improvement for 10 epochs)\n",
      "======\n",
      "Best epoch 49 \t val_acc 0.3243 \t macro_f1 0.5404\n",
      "CPU times: user 24min 11s, sys: 29min 55s, total: 54min 6s\n",
      "Wall time: 7min 5s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "run_experiment('mlp', use_img=True, use_txt=True, use_loc=False, use_mixup=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f776a0e-0899-4056-b701-4ba2348aeebc",
   "metadata": {},
   "source": [
    "### Image + location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "01d24faa-d44b-4db7-8df1-0ccee0b8de75",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00 \t train_loss 0.1354 \t val_acc 0.1750 \t macro_f1 0.3152\n",
      "Epoch 01 \t train_loss 0.1045 \t val_acc 0.1982 \t macro_f1 0.3716\n",
      "Epoch 02 \t train_loss 0.1022 \t val_acc 0.2066 \t macro_f1 0.3900\n",
      "Epoch 03 \t train_loss 0.1012 \t val_acc 0.2120 \t macro_f1 0.4019\n",
      "Epoch 04 \t train_loss 0.1005 \t val_acc 0.2151 \t macro_f1 0.4084\n",
      "Epoch 05 \t train_loss 0.1001 \t val_acc 0.2156 \t macro_f1 0.4090\n",
      "Epoch 06 \t train_loss 0.0998 \t val_acc 0.2178 \t macro_f1 0.4138\n",
      "Epoch 07 \t train_loss 0.0996 \t val_acc 0.2194 \t macro_f1 0.4151\n",
      "Epoch 08 \t train_loss 0.0994 \t val_acc 0.2199 \t macro_f1 0.4161\n",
      "Epoch 09 \t train_loss 0.0992 \t val_acc 0.2204 \t macro_f1 0.4170\n",
      "Epoch 10 \t train_loss 0.0991 \t val_acc 0.2205 \t macro_f1 0.4207\n",
      "Epoch 11 \t train_loss 0.0990 \t val_acc 0.2219 \t macro_f1 0.4224\n",
      "Epoch 12 \t train_loss 0.0989 \t val_acc 0.2225 \t macro_f1 0.4247\n",
      "Epoch 13 \t train_loss 0.0989 \t val_acc 0.2232 \t macro_f1 0.4224\n",
      "Epoch 14 \t train_loss 0.0988 \t val_acc 0.2241 \t macro_f1 0.4270\n",
      "Epoch 15 \t train_loss 0.0987 \t val_acc 0.2239 \t macro_f1 0.4267\n",
      "Epoch 16 \t train_loss 0.0987 \t val_acc 0.2238 \t macro_f1 0.4266\n",
      "Epoch 17 \t train_loss 0.0986 \t val_acc 0.2252 \t macro_f1 0.4284\n",
      "Epoch 18 \t train_loss 0.0986 \t val_acc 0.2249 \t macro_f1 0.4290\n",
      "Epoch 19 \t train_loss 0.0985 \t val_acc 0.2252 \t macro_f1 0.4309\n",
      "Epoch 20 \t train_loss 0.0985 \t val_acc 0.2254 \t macro_f1 0.4275\n",
      "Epoch 21 \t train_loss 0.0985 \t val_acc 0.2250 \t macro_f1 0.4289\n",
      "Epoch 22 \t train_loss 0.0984 \t val_acc 0.2241 \t macro_f1 0.4273\n",
      "Epoch 23 \t train_loss 0.0984 \t val_acc 0.2265 \t macro_f1 0.4304\n",
      "Epoch 24 \t train_loss 0.0984 \t val_acc 0.2258 \t macro_f1 0.4324\n",
      "Epoch 25 \t train_loss 0.0984 \t val_acc 0.2258 \t macro_f1 0.4311\n",
      "Epoch 26 \t train_loss 0.0983 \t val_acc 0.2261 \t macro_f1 0.4306\n",
      "Epoch 27 \t train_loss 0.0983 \t val_acc 0.2259 \t macro_f1 0.4285\n",
      "Epoch 28 \t train_loss 0.0982 \t val_acc 0.2263 \t macro_f1 0.4311\n",
      "Epoch 29 \t train_loss 0.0983 \t val_acc 0.2265 \t macro_f1 0.4304\n",
      "Epoch 30 \t train_loss 0.0982 \t val_acc 0.2248 \t macro_f1 0.4273\n",
      "Epoch 31 \t train_loss 0.0982 \t val_acc 0.2264 \t macro_f1 0.4298\n",
      "Epoch 32 \t train_loss 0.0982 \t val_acc 0.2261 \t macro_f1 0.4286\n",
      "Epoch 33 \t train_loss 0.0982 \t val_acc 0.2257 \t macro_f1 0.4309\n",
      "Early stopping at epoch 33 (no improvement for 10 epochs)\n",
      "======\n",
      "Best epoch 33 \t val_acc 0.2265 \t macro_f1 0.4304\n",
      "CPU times: user 9min 52s, sys: 21min 19s, total: 31min 12s\n",
      "Wall time: 4min 29s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "run_experiment('linear', use_img=True, use_txt=False, use_loc=True, use_mixup=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0b63b22f-b28c-4f2d-9bdf-eb6eae5d773f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00 \t train_loss 0.1283 \t val_acc 0.1731 \t macro_f1 0.3068\n",
      "Epoch 01 \t train_loss 0.0953 \t val_acc 0.1950 \t macro_f1 0.3564\n",
      "Epoch 02 \t train_loss 0.0926 \t val_acc 0.2026 \t macro_f1 0.3739\n",
      "Epoch 03 \t train_loss 0.0914 \t val_acc 0.2077 \t macro_f1 0.3835\n",
      "Epoch 04 \t train_loss 0.0907 \t val_acc 0.2093 \t macro_f1 0.3883\n",
      "Epoch 05 \t train_loss 0.0902 \t val_acc 0.2117 \t macro_f1 0.3944\n",
      "Epoch 06 \t train_loss 0.0898 \t val_acc 0.2136 \t macro_f1 0.3963\n",
      "Epoch 07 \t train_loss 0.0895 \t val_acc 0.2150 \t macro_f1 0.3978\n",
      "Epoch 08 \t train_loss 0.0893 \t val_acc 0.2144 \t macro_f1 0.3971\n",
      "Epoch 09 \t train_loss 0.0891 \t val_acc 0.2170 \t macro_f1 0.4018\n",
      "Epoch 10 \t train_loss 0.0889 \t val_acc 0.2190 \t macro_f1 0.4055\n",
      "Epoch 11 \t train_loss 0.0888 \t val_acc 0.2168 \t macro_f1 0.4018\n",
      "Epoch 12 \t train_loss 0.0887 \t val_acc 0.2189 \t macro_f1 0.4057\n",
      "Epoch 13 \t train_loss 0.0886 \t val_acc 0.2178 \t macro_f1 0.4049\n",
      "Epoch 14 \t train_loss 0.0885 \t val_acc 0.2183 \t macro_f1 0.4065\n",
      "Epoch 15 \t train_loss 0.0884 \t val_acc 0.2200 \t macro_f1 0.4081\n",
      "Epoch 16 \t train_loss 0.0883 \t val_acc 0.2202 \t macro_f1 0.4077\n",
      "Epoch 17 \t train_loss 0.0883 \t val_acc 0.2205 \t macro_f1 0.4089\n",
      "Epoch 18 \t train_loss 0.0882 \t val_acc 0.2202 \t macro_f1 0.4094\n",
      "Epoch 19 \t train_loss 0.0882 \t val_acc 0.2199 \t macro_f1 0.4076\n",
      "Epoch 20 \t train_loss 0.0881 \t val_acc 0.2205 \t macro_f1 0.4081\n",
      "Epoch 21 \t train_loss 0.0881 \t val_acc 0.2207 \t macro_f1 0.4095\n",
      "Epoch 22 \t train_loss 0.0880 \t val_acc 0.2209 \t macro_f1 0.4083\n",
      "Epoch 23 \t train_loss 0.0880 \t val_acc 0.2209 \t macro_f1 0.4100\n",
      "Epoch 24 \t train_loss 0.0880 \t val_acc 0.2213 \t macro_f1 0.4089\n",
      "Epoch 25 \t train_loss 0.0879 \t val_acc 0.2222 \t macro_f1 0.4130\n",
      "Epoch 26 \t train_loss 0.0879 \t val_acc 0.2207 \t macro_f1 0.4097\n",
      "Epoch 27 \t train_loss 0.0879 \t val_acc 0.2214 \t macro_f1 0.4096\n",
      "Epoch 28 \t train_loss 0.0878 \t val_acc 0.2217 \t macro_f1 0.4102\n",
      "Epoch 29 \t train_loss 0.0878 \t val_acc 0.2222 \t macro_f1 0.4093\n",
      "Epoch 30 \t train_loss 0.0878 \t val_acc 0.2216 \t macro_f1 0.4113\n",
      "Epoch 31 \t train_loss 0.0878 \t val_acc 0.2228 \t macro_f1 0.4112\n",
      "Epoch 32 \t train_loss 0.0877 \t val_acc 0.2217 \t macro_f1 0.4117\n",
      "Epoch 33 \t train_loss 0.0877 \t val_acc 0.2221 \t macro_f1 0.4126\n",
      "Epoch 34 \t train_loss 0.0877 \t val_acc 0.2221 \t macro_f1 0.4108\n",
      "Epoch 35 \t train_loss 0.0877 \t val_acc 0.2219 \t macro_f1 0.4110\n",
      "Epoch 36 \t train_loss 0.0876 \t val_acc 0.2223 \t macro_f1 0.4129\n",
      "Epoch 37 \t train_loss 0.0876 \t val_acc 0.2224 \t macro_f1 0.4116\n",
      "Epoch 38 \t train_loss 0.0876 \t val_acc 0.2220 \t macro_f1 0.4115\n",
      "Epoch 39 \t train_loss 0.0876 \t val_acc 0.2224 \t macro_f1 0.4116\n",
      "Epoch 40 \t train_loss 0.0876 \t val_acc 0.2221 \t macro_f1 0.4111\n",
      "Epoch 41 \t train_loss 0.0876 \t val_acc 0.2222 \t macro_f1 0.4120\n",
      "Early stopping at epoch 41 (no improvement for 10 epochs)\n",
      "======\n",
      "Best epoch 41 \t val_acc 0.2228 \t macro_f1 0.4112\n",
      "CPU times: user 15min 5s, sys: 21min 15s, total: 36min 20s\n",
      "Wall time: 5min 5s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "run_experiment('linear', use_img=True, use_txt=False, use_loc=True, use_mixup=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "af12f0e9-1a13-46ad-bd1c-b7bda87b9702",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00 \t train_loss 0.1509 \t val_acc 0.1353 \t macro_f1 0.2017\n",
      "Epoch 01 \t train_loss 0.1109 \t val_acc 0.1655 \t macro_f1 0.2599\n",
      "Epoch 02 \t train_loss 0.1078 \t val_acc 0.1743 \t macro_f1 0.2810\n",
      "Epoch 03 \t train_loss 0.1065 \t val_acc 0.1829 \t macro_f1 0.2968\n",
      "Epoch 04 \t train_loss 0.1057 \t val_acc 0.1880 \t macro_f1 0.3094\n",
      "Epoch 05 \t train_loss 0.1052 \t val_acc 0.1859 \t macro_f1 0.3059\n",
      "Epoch 06 \t train_loss 0.1047 \t val_acc 0.1904 \t macro_f1 0.3163\n",
      "Epoch 07 \t train_loss 0.1044 \t val_acc 0.1924 \t macro_f1 0.3230\n",
      "Epoch 08 \t train_loss 0.1041 \t val_acc 0.1934 \t macro_f1 0.3254\n",
      "Epoch 09 \t train_loss 0.1038 \t val_acc 0.1954 \t macro_f1 0.3312\n",
      "Epoch 10 \t train_loss 0.1036 \t val_acc 0.1939 \t macro_f1 0.3274\n",
      "Epoch 11 \t train_loss 0.1034 \t val_acc 0.1982 \t macro_f1 0.3352\n",
      "Epoch 12 \t train_loss 0.1032 \t val_acc 0.1953 \t macro_f1 0.3315\n",
      "Epoch 13 \t train_loss 0.1030 \t val_acc 0.1959 \t macro_f1 0.3317\n",
      "Epoch 14 \t train_loss 0.1028 \t val_acc 0.1992 \t macro_f1 0.3387\n",
      "Epoch 15 \t train_loss 0.1026 \t val_acc 0.2010 \t macro_f1 0.3410\n",
      "Epoch 16 \t train_loss 0.1025 \t val_acc 0.1995 \t macro_f1 0.3412\n",
      "Epoch 17 \t train_loss 0.1023 \t val_acc 0.1991 \t macro_f1 0.3378\n",
      "Epoch 18 \t train_loss 0.1021 \t val_acc 0.2026 \t macro_f1 0.3473\n",
      "Epoch 19 \t train_loss 0.1020 \t val_acc 0.2038 \t macro_f1 0.3502\n",
      "Epoch 20 \t train_loss 0.1019 \t val_acc 0.2014 \t macro_f1 0.3475\n",
      "Epoch 21 \t train_loss 0.1017 \t val_acc 0.2038 \t macro_f1 0.3495\n",
      "Epoch 22 \t train_loss 0.1016 \t val_acc 0.2037 \t macro_f1 0.3504\n",
      "Epoch 23 \t train_loss 0.1015 \t val_acc 0.2039 \t macro_f1 0.3496\n",
      "Epoch 24 \t train_loss 0.1013 \t val_acc 0.2047 \t macro_f1 0.3509\n",
      "Epoch 25 \t train_loss 0.1013 \t val_acc 0.2054 \t macro_f1 0.3552\n",
      "Epoch 26 \t train_loss 0.1011 \t val_acc 0.2043 \t macro_f1 0.3494\n",
      "Epoch 27 \t train_loss 0.1010 \t val_acc 0.2056 \t macro_f1 0.3553\n",
      "Epoch 28 \t train_loss 0.1009 \t val_acc 0.2062 \t macro_f1 0.3538\n",
      "Epoch 29 \t train_loss 0.1009 \t val_acc 0.2058 \t macro_f1 0.3547\n",
      "Epoch 30 \t train_loss 0.1008 \t val_acc 0.2077 \t macro_f1 0.3564\n",
      "Epoch 31 \t train_loss 0.1006 \t val_acc 0.2077 \t macro_f1 0.3579\n",
      "Epoch 32 \t train_loss 0.1006 \t val_acc 0.2079 \t macro_f1 0.3546\n",
      "Epoch 33 \t train_loss 0.1005 \t val_acc 0.2071 \t macro_f1 0.3565\n",
      "Epoch 34 \t train_loss 0.1004 \t val_acc 0.2088 \t macro_f1 0.3612\n",
      "Epoch 35 \t train_loss 0.1003 \t val_acc 0.2079 \t macro_f1 0.3569\n",
      "Epoch 36 \t train_loss 0.1003 \t val_acc 0.2085 \t macro_f1 0.3590\n",
      "Epoch 37 \t train_loss 0.1002 \t val_acc 0.2096 \t macro_f1 0.3604\n",
      "Epoch 38 \t train_loss 0.1002 \t val_acc 0.2091 \t macro_f1 0.3607\n",
      "Epoch 39 \t train_loss 0.1001 \t val_acc 0.2089 \t macro_f1 0.3602\n",
      "Epoch 40 \t train_loss 0.1001 \t val_acc 0.2092 \t macro_f1 0.3622\n",
      "Epoch 41 \t train_loss 0.1000 \t val_acc 0.2099 \t macro_f1 0.3621\n",
      "Epoch 42 \t train_loss 0.1001 \t val_acc 0.2093 \t macro_f1 0.3611\n",
      "Epoch 43 \t train_loss 0.1000 \t val_acc 0.2096 \t macro_f1 0.3612\n",
      "Epoch 44 \t train_loss 0.1000 \t val_acc 0.2098 \t macro_f1 0.3626\n",
      "Epoch 45 \t train_loss 0.1000 \t val_acc 0.2095 \t macro_f1 0.3616\n",
      "Epoch 46 \t train_loss 0.0999 \t val_acc 0.2096 \t macro_f1 0.3618\n",
      "Epoch 47 \t train_loss 0.1000 \t val_acc 0.2098 \t macro_f1 0.3620\n",
      "Epoch 48 \t train_loss 0.0999 \t val_acc 0.2097 \t macro_f1 0.3621\n",
      "Epoch 49 \t train_loss 0.0999 \t val_acc 0.2097 \t macro_f1 0.3621\n",
      "Epoch 50 \t train_loss 0.1000 \t val_acc 0.2097 \t macro_f1 0.3621\n",
      "Epoch 51 \t train_loss 0.0999 \t val_acc 0.2098 \t macro_f1 0.3623\n",
      "Early stopping at epoch 51 (no improvement for 10 epochs)\n",
      "======\n",
      "Best epoch 51 \t val_acc 0.2099 \t macro_f1 0.3621\n",
      "CPU times: user 16min 38s, sys: 36min 59s, total: 53min 37s\n",
      "Wall time: 7min 48s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "run_experiment('mlp', use_img=True, use_txt=False, use_loc=True, use_mixup=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7f7eccf4-838e-4eb0-beb5-b102635e1bf5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00 \t train_loss 0.1421 \t val_acc 0.1425 \t macro_f1 0.2184\n",
      "Epoch 01 \t train_loss 0.1016 \t val_acc 0.1769 \t macro_f1 0.2824\n",
      "Epoch 02 \t train_loss 0.0982 \t val_acc 0.1846 \t macro_f1 0.3019\n",
      "Epoch 03 \t train_loss 0.0967 \t val_acc 0.1928 \t macro_f1 0.3181\n",
      "Epoch 04 \t train_loss 0.0957 \t val_acc 0.1971 \t macro_f1 0.3300\n",
      "Epoch 05 \t train_loss 0.0950 \t val_acc 0.2004 \t macro_f1 0.3385\n",
      "Epoch 06 \t train_loss 0.0945 \t val_acc 0.2027 \t macro_f1 0.3437\n",
      "Epoch 07 \t train_loss 0.0941 \t val_acc 0.2032 \t macro_f1 0.3480\n",
      "Epoch 08 \t train_loss 0.0937 \t val_acc 0.2055 \t macro_f1 0.3500\n",
      "Epoch 09 \t train_loss 0.0934 \t val_acc 0.2066 \t macro_f1 0.3528\n",
      "Epoch 10 \t train_loss 0.0931 \t val_acc 0.2064 \t macro_f1 0.3517\n",
      "Epoch 11 \t train_loss 0.0929 \t val_acc 0.2086 \t macro_f1 0.3585\n",
      "Epoch 12 \t train_loss 0.0927 \t val_acc 0.2105 \t macro_f1 0.3585\n",
      "Epoch 13 \t train_loss 0.0924 \t val_acc 0.2116 \t macro_f1 0.3604\n",
      "Epoch 14 \t train_loss 0.0923 \t val_acc 0.2110 \t macro_f1 0.3618\n",
      "Epoch 15 \t train_loss 0.0921 \t val_acc 0.2110 \t macro_f1 0.3605\n",
      "Epoch 16 \t train_loss 0.0920 \t val_acc 0.2123 \t macro_f1 0.3623\n",
      "Epoch 17 \t train_loss 0.0918 \t val_acc 0.2127 \t macro_f1 0.3665\n",
      "Epoch 18 \t train_loss 0.0917 \t val_acc 0.2104 \t macro_f1 0.3610\n",
      "Epoch 19 \t train_loss 0.0915 \t val_acc 0.2142 \t macro_f1 0.3684\n",
      "Epoch 20 \t train_loss 0.0914 \t val_acc 0.2124 \t macro_f1 0.3654\n",
      "Epoch 21 \t train_loss 0.0913 \t val_acc 0.2129 \t macro_f1 0.3664\n",
      "Epoch 22 \t train_loss 0.0912 \t val_acc 0.2138 \t macro_f1 0.3704\n",
      "Epoch 23 \t train_loss 0.0911 \t val_acc 0.2131 \t macro_f1 0.3698\n",
      "Epoch 24 \t train_loss 0.0910 \t val_acc 0.2145 \t macro_f1 0.3697\n",
      "Epoch 25 \t train_loss 0.0909 \t val_acc 0.2151 \t macro_f1 0.3679\n",
      "Epoch 26 \t train_loss 0.0908 \t val_acc 0.2161 \t macro_f1 0.3739\n",
      "Epoch 27 \t train_loss 0.0907 \t val_acc 0.2142 \t macro_f1 0.3691\n",
      "Epoch 28 \t train_loss 0.0906 \t val_acc 0.2145 \t macro_f1 0.3712\n",
      "Epoch 29 \t train_loss 0.0905 \t val_acc 0.2169 \t macro_f1 0.3740\n",
      "Epoch 30 \t train_loss 0.0905 \t val_acc 0.2177 \t macro_f1 0.3741\n",
      "Epoch 31 \t train_loss 0.0904 \t val_acc 0.2161 \t macro_f1 0.3735\n",
      "Epoch 32 \t train_loss 0.0903 \t val_acc 0.2154 \t macro_f1 0.3722\n",
      "Epoch 33 \t train_loss 0.0903 \t val_acc 0.2169 \t macro_f1 0.3749\n",
      "Epoch 34 \t train_loss 0.0902 \t val_acc 0.2167 \t macro_f1 0.3744\n",
      "Epoch 35 \t train_loss 0.0901 \t val_acc 0.2173 \t macro_f1 0.3744\n",
      "Epoch 36 \t train_loss 0.0901 \t val_acc 0.2182 \t macro_f1 0.3759\n",
      "Epoch 37 \t train_loss 0.0900 \t val_acc 0.2174 \t macro_f1 0.3760\n",
      "Epoch 38 \t train_loss 0.0900 \t val_acc 0.2182 \t macro_f1 0.3761\n",
      "Epoch 39 \t train_loss 0.0899 \t val_acc 0.2183 \t macro_f1 0.3745\n",
      "Epoch 40 \t train_loss 0.0899 \t val_acc 0.2175 \t macro_f1 0.3741\n",
      "Epoch 41 \t train_loss 0.0899 \t val_acc 0.2179 \t macro_f1 0.3762\n",
      "Epoch 42 \t train_loss 0.0899 \t val_acc 0.2181 \t macro_f1 0.3764\n",
      "Epoch 43 \t train_loss 0.0898 \t val_acc 0.2180 \t macro_f1 0.3762\n",
      "Epoch 44 \t train_loss 0.0898 \t val_acc 0.2175 \t macro_f1 0.3761\n",
      "Epoch 45 \t train_loss 0.0898 \t val_acc 0.2181 \t macro_f1 0.3760\n",
      "Epoch 46 \t train_loss 0.0897 \t val_acc 0.2182 \t macro_f1 0.3762\n",
      "Epoch 47 \t train_loss 0.0898 \t val_acc 0.2183 \t macro_f1 0.3761\n",
      "Epoch 48 \t train_loss 0.0898 \t val_acc 0.2182 \t macro_f1 0.3761\n",
      "Epoch 49 \t train_loss 0.0897 \t val_acc 0.2182 \t macro_f1 0.3760\n",
      "Early stopping at epoch 49 (no improvement for 10 epochs)\n",
      "======\n",
      "Best epoch 49 \t val_acc 0.2183 \t macro_f1 0.3745\n",
      "CPU times: user 20min 31s, sys: 29min 14s, total: 49min 46s\n",
      "Wall time: 6min 53s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "run_experiment('mlp', use_img=True, use_txt=False, use_loc=True, use_mixup=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae2eafe0-f248-4616-85be-8c0b2ddd8a8a",
   "metadata": {},
   "source": [
    "### Title + location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fda23e57-e933-45cf-b313-9b782bd7d337",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00 \t train_loss 0.1438 \t val_acc 0.2052 \t macro_f1 0.3735\n",
      "Epoch 01 \t train_loss 0.1039 \t val_acc 0.2434 \t macro_f1 0.4531\n",
      "Epoch 02 \t train_loss 0.1007 \t val_acc 0.2537 \t macro_f1 0.4745\n",
      "Epoch 03 \t train_loss 0.0994 \t val_acc 0.2607 \t macro_f1 0.4860\n",
      "Epoch 04 \t train_loss 0.0986 \t val_acc 0.2644 \t macro_f1 0.4930\n",
      "Epoch 05 \t train_loss 0.0981 \t val_acc 0.2667 \t macro_f1 0.4963\n",
      "Epoch 06 \t train_loss 0.0978 \t val_acc 0.2688 \t macro_f1 0.5010\n",
      "Epoch 07 \t train_loss 0.0974 \t val_acc 0.2704 \t macro_f1 0.5025\n",
      "Epoch 08 \t train_loss 0.0972 \t val_acc 0.2715 \t macro_f1 0.5049\n",
      "Epoch 09 \t train_loss 0.0970 \t val_acc 0.2718 \t macro_f1 0.5076\n",
      "Epoch 10 \t train_loss 0.0968 \t val_acc 0.2731 \t macro_f1 0.5085\n",
      "Epoch 11 \t train_loss 0.0967 \t val_acc 0.2731 \t macro_f1 0.5096\n",
      "Epoch 12 \t train_loss 0.0966 \t val_acc 0.2734 \t macro_f1 0.5103\n",
      "Epoch 13 \t train_loss 0.0965 \t val_acc 0.2753 \t macro_f1 0.5113\n",
      "Epoch 14 \t train_loss 0.0964 \t val_acc 0.2751 \t macro_f1 0.5123\n",
      "Epoch 15 \t train_loss 0.0965 \t val_acc 0.2761 \t macro_f1 0.5131\n",
      "Epoch 16 \t train_loss 0.0964 \t val_acc 0.2762 \t macro_f1 0.5144\n",
      "Epoch 17 \t train_loss 0.0963 \t val_acc 0.2762 \t macro_f1 0.5130\n",
      "Epoch 18 \t train_loss 0.0962 \t val_acc 0.2759 \t macro_f1 0.5149\n",
      "Epoch 19 \t train_loss 0.0961 \t val_acc 0.2768 \t macro_f1 0.5130\n",
      "Epoch 20 \t train_loss 0.0961 \t val_acc 0.2767 \t macro_f1 0.5156\n",
      "Epoch 21 \t train_loss 0.0961 \t val_acc 0.2774 \t macro_f1 0.5150\n",
      "Epoch 22 \t train_loss 0.0961 \t val_acc 0.2774 \t macro_f1 0.5150\n",
      "Epoch 23 \t train_loss 0.0961 \t val_acc 0.2779 \t macro_f1 0.5162\n",
      "Epoch 24 \t train_loss 0.0960 \t val_acc 0.2780 \t macro_f1 0.5168\n",
      "Epoch 25 \t train_loss 0.0960 \t val_acc 0.2782 \t macro_f1 0.5161\n",
      "Epoch 26 \t train_loss 0.0960 \t val_acc 0.2782 \t macro_f1 0.5161\n",
      "Epoch 27 \t train_loss 0.0960 \t val_acc 0.2783 \t macro_f1 0.5164\n",
      "Epoch 28 \t train_loss 0.0959 \t val_acc 0.2786 \t macro_f1 0.5163\n",
      "Epoch 29 \t train_loss 0.0960 \t val_acc 0.2783 \t macro_f1 0.5160\n",
      "Epoch 30 \t train_loss 0.0959 \t val_acc 0.2787 \t macro_f1 0.5166\n",
      "Epoch 31 \t train_loss 0.0959 \t val_acc 0.2785 \t macro_f1 0.5159\n",
      "Epoch 32 \t train_loss 0.0959 \t val_acc 0.2782 \t macro_f1 0.5169\n",
      "Epoch 33 \t train_loss 0.0958 \t val_acc 0.2784 \t macro_f1 0.5171\n",
      "Epoch 34 \t train_loss 0.0958 \t val_acc 0.2789 \t macro_f1 0.5176\n",
      "Epoch 35 \t train_loss 0.0958 \t val_acc 0.2783 \t macro_f1 0.5178\n",
      "Epoch 36 \t train_loss 0.0958 \t val_acc 0.2782 \t macro_f1 0.5169\n",
      "Epoch 37 \t train_loss 0.0958 \t val_acc 0.2792 \t macro_f1 0.5175\n",
      "Epoch 38 \t train_loss 0.0958 \t val_acc 0.2783 \t macro_f1 0.5176\n",
      "Epoch 39 \t train_loss 0.0958 \t val_acc 0.2788 \t macro_f1 0.5180\n",
      "Epoch 40 \t train_loss 0.0958 \t val_acc 0.2789 \t macro_f1 0.5180\n",
      "Epoch 41 \t train_loss 0.0958 \t val_acc 0.2789 \t macro_f1 0.5174\n",
      "Epoch 42 \t train_loss 0.0957 \t val_acc 0.2790 \t macro_f1 0.5181\n",
      "Epoch 43 \t train_loss 0.0957 \t val_acc 0.2791 \t macro_f1 0.5176\n",
      "Epoch 44 \t train_loss 0.0957 \t val_acc 0.2790 \t macro_f1 0.5180\n",
      "Epoch 45 \t train_loss 0.0957 \t val_acc 0.2790 \t macro_f1 0.5180\n",
      "Epoch 46 \t train_loss 0.0957 \t val_acc 0.2790 \t macro_f1 0.5178\n",
      "Epoch 47 \t train_loss 0.0957 \t val_acc 0.2791 \t macro_f1 0.5180\n",
      "Early stopping at epoch 47 (no improvement for 10 epochs)\n",
      "======\n",
      "Best epoch 47 \t val_acc 0.2792 \t macro_f1 0.5175\n",
      "CPU times: user 13min 43s, sys: 29min 47s, total: 43min 30s\n",
      "Wall time: 6min 20s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "run_experiment('linear', use_img=False, use_txt=True, use_loc=True, use_mixup=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7ff41fdd-7dd3-40aa-af91-61865a723539",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00 \t train_loss 0.1384 \t val_acc 0.2035 \t macro_f1 0.3670\n",
      "Epoch 01 \t train_loss 0.0948 \t val_acc 0.2386 \t macro_f1 0.4397\n",
      "Epoch 02 \t train_loss 0.0913 \t val_acc 0.2505 \t macro_f1 0.4611\n",
      "Epoch 03 \t train_loss 0.0897 \t val_acc 0.2568 \t macro_f1 0.4718\n",
      "Epoch 04 \t train_loss 0.0888 \t val_acc 0.2606 \t macro_f1 0.4798\n",
      "Epoch 05 \t train_loss 0.0882 \t val_acc 0.2633 \t macro_f1 0.4832\n",
      "Epoch 06 \t train_loss 0.0877 \t val_acc 0.2655 \t macro_f1 0.4866\n",
      "Epoch 07 \t train_loss 0.0874 \t val_acc 0.2666 \t macro_f1 0.4900\n",
      "Epoch 08 \t train_loss 0.0871 \t val_acc 0.2686 \t macro_f1 0.4939\n",
      "Epoch 09 \t train_loss 0.0869 \t val_acc 0.2691 \t macro_f1 0.4915\n",
      "Epoch 10 \t train_loss 0.0867 \t val_acc 0.2698 \t macro_f1 0.4944\n",
      "Epoch 11 \t train_loss 0.0866 \t val_acc 0.2711 \t macro_f1 0.4971\n",
      "Epoch 12 \t train_loss 0.0864 \t val_acc 0.2705 \t macro_f1 0.4958\n",
      "Epoch 13 \t train_loss 0.0863 \t val_acc 0.2715 \t macro_f1 0.4983\n",
      "Epoch 14 \t train_loss 0.0862 \t val_acc 0.2719 \t macro_f1 0.4986\n",
      "Epoch 15 \t train_loss 0.0861 \t val_acc 0.2737 \t macro_f1 0.5003\n",
      "Epoch 16 \t train_loss 0.0861 \t val_acc 0.2730 \t macro_f1 0.4992\n",
      "Epoch 17 \t train_loss 0.0860 \t val_acc 0.2730 \t macro_f1 0.5003\n",
      "Epoch 18 \t train_loss 0.0859 \t val_acc 0.2723 \t macro_f1 0.4990\n",
      "Epoch 19 \t train_loss 0.0859 \t val_acc 0.2728 \t macro_f1 0.5003\n",
      "Epoch 20 \t train_loss 0.0859 \t val_acc 0.2742 \t macro_f1 0.5011\n",
      "Epoch 21 \t train_loss 0.0858 \t val_acc 0.2736 \t macro_f1 0.5012\n",
      "Epoch 22 \t train_loss 0.0858 \t val_acc 0.2743 \t macro_f1 0.5020\n",
      "Epoch 23 \t train_loss 0.0857 \t val_acc 0.2744 \t macro_f1 0.5028\n",
      "Epoch 24 \t train_loss 0.0857 \t val_acc 0.2754 \t macro_f1 0.5038\n",
      "Epoch 25 \t train_loss 0.0857 \t val_acc 0.2744 \t macro_f1 0.5027\n",
      "Epoch 26 \t train_loss 0.0856 \t val_acc 0.2747 \t macro_f1 0.5021\n",
      "Epoch 27 \t train_loss 0.0856 \t val_acc 0.2747 \t macro_f1 0.5027\n",
      "Epoch 28 \t train_loss 0.0856 \t val_acc 0.2753 \t macro_f1 0.5035\n",
      "Epoch 29 \t train_loss 0.0856 \t val_acc 0.2753 \t macro_f1 0.5037\n",
      "Epoch 30 \t train_loss 0.0855 \t val_acc 0.2753 \t macro_f1 0.5034\n",
      "Epoch 31 \t train_loss 0.0855 \t val_acc 0.2750 \t macro_f1 0.5027\n",
      "Epoch 32 \t train_loss 0.0855 \t val_acc 0.2746 \t macro_f1 0.5023\n",
      "Epoch 33 \t train_loss 0.0855 \t val_acc 0.2759 \t macro_f1 0.5049\n",
      "Epoch 34 \t train_loss 0.0855 \t val_acc 0.2754 \t macro_f1 0.5038\n",
      "Epoch 35 \t train_loss 0.0854 \t val_acc 0.2753 \t macro_f1 0.5039\n",
      "Epoch 36 \t train_loss 0.0854 \t val_acc 0.2757 \t macro_f1 0.5036\n",
      "Epoch 37 \t train_loss 0.0854 \t val_acc 0.2755 \t macro_f1 0.5035\n",
      "Epoch 38 \t train_loss 0.0854 \t val_acc 0.2760 \t macro_f1 0.5035\n",
      "Epoch 39 \t train_loss 0.0854 \t val_acc 0.2754 \t macro_f1 0.5036\n",
      "Epoch 40 \t train_loss 0.0854 \t val_acc 0.2759 \t macro_f1 0.5044\n",
      "Epoch 41 \t train_loss 0.0854 \t val_acc 0.2759 \t macro_f1 0.5040\n",
      "Epoch 42 \t train_loss 0.0853 \t val_acc 0.2759 \t macro_f1 0.5042\n",
      "Epoch 43 \t train_loss 0.0853 \t val_acc 0.2760 \t macro_f1 0.5042\n",
      "Epoch 44 \t train_loss 0.0853 \t val_acc 0.2761 \t macro_f1 0.5039\n",
      "Epoch 45 \t train_loss 0.0853 \t val_acc 0.2760 \t macro_f1 0.5040\n",
      "Epoch 46 \t train_loss 0.0853 \t val_acc 0.2761 \t macro_f1 0.5040\n",
      "Epoch 47 \t train_loss 0.0853 \t val_acc 0.2761 \t macro_f1 0.5042\n",
      "Epoch 48 \t train_loss 0.0853 \t val_acc 0.2761 \t macro_f1 0.5042\n",
      "Epoch 49 \t train_loss 0.0853 \t val_acc 0.2761 \t macro_f1 0.5042\n",
      "Epoch 50 \t train_loss 0.0853 \t val_acc 0.2761 \t macro_f1 0.5042\n",
      "Epoch 51 \t train_loss 0.0853 \t val_acc 0.2762 \t macro_f1 0.5042\n",
      "Epoch 52 \t train_loss 0.0853 \t val_acc 0.2761 \t macro_f1 0.5042\n",
      "Epoch 53 \t train_loss 0.0853 \t val_acc 0.2761 \t macro_f1 0.5043\n",
      "Epoch 54 \t train_loss 0.0853 \t val_acc 0.2761 \t macro_f1 0.5043\n",
      "Epoch 55 \t train_loss 0.0853 \t val_acc 0.2760 \t macro_f1 0.5043\n",
      "Epoch 56 \t train_loss 0.0853 \t val_acc 0.2762 \t macro_f1 0.5042\n",
      "Epoch 57 \t train_loss 0.0853 \t val_acc 0.2760 \t macro_f1 0.5043\n",
      "Epoch 58 \t train_loss 0.0853 \t val_acc 0.2759 \t macro_f1 0.5046\n",
      "Epoch 59 \t train_loss 0.0853 \t val_acc 0.2759 \t macro_f1 0.5046\n",
      "Epoch 60 \t train_loss 0.0853 \t val_acc 0.2760 \t macro_f1 0.5032\n",
      "Epoch 61 \t train_loss 0.0854 \t val_acc 0.2758 \t macro_f1 0.5039\n",
      "Epoch 62 \t train_loss 0.0854 \t val_acc 0.2762 \t macro_f1 0.5048\n",
      "Epoch 63 \t train_loss 0.0854 \t val_acc 0.2760 \t macro_f1 0.5027\n",
      "Epoch 64 \t train_loss 0.0854 \t val_acc 0.2760 \t macro_f1 0.5045\n",
      "Epoch 65 \t train_loss 0.0854 \t val_acc 0.2764 \t macro_f1 0.5038\n",
      "Epoch 66 \t train_loss 0.0854 \t val_acc 0.2761 \t macro_f1 0.5058\n",
      "Epoch 67 \t train_loss 0.0854 \t val_acc 0.2757 \t macro_f1 0.5045\n",
      "Epoch 68 \t train_loss 0.0854 \t val_acc 0.2761 \t macro_f1 0.5049\n",
      "Epoch 69 \t train_loss 0.0854 \t val_acc 0.2760 \t macro_f1 0.5039\n",
      "Epoch 70 \t train_loss 0.0854 \t val_acc 0.2757 \t macro_f1 0.5039\n",
      "Epoch 71 \t train_loss 0.0854 \t val_acc 0.2759 \t macro_f1 0.5048\n",
      "Epoch 72 \t train_loss 0.0854 \t val_acc 0.2762 \t macro_f1 0.5044\n",
      "Epoch 73 \t train_loss 0.0854 \t val_acc 0.2761 \t macro_f1 0.5028\n",
      "Epoch 74 \t train_loss 0.0854 \t val_acc 0.2763 \t macro_f1 0.5049\n",
      "Epoch 75 \t train_loss 0.0854 \t val_acc 0.2753 \t macro_f1 0.5021\n",
      "Early stopping at epoch 75 (no improvement for 10 epochs)\n",
      "======\n",
      "Best epoch 75 \t val_acc 0.2764 \t macro_f1 0.5038\n",
      "CPU times: user 29min 10s, sys: 38min 15s, total: 1h 7min 26s\n",
      "Wall time: 8min 59s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "run_experiment('linear', use_img=False, use_txt=True, use_loc=True, use_mixup=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4ef5d58a-f926-4eaa-881a-46350cc63987",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00 \t train_loss 0.1531 \t val_acc 0.1714 \t macro_f1 0.2830\n",
      "Epoch 01 \t train_loss 0.1084 \t val_acc 0.2212 \t macro_f1 0.3805\n",
      "Epoch 02 \t train_loss 0.1046 \t val_acc 0.2338 \t macro_f1 0.4068\n",
      "Epoch 03 \t train_loss 0.1031 \t val_acc 0.2424 \t macro_f1 0.4234\n",
      "Epoch 04 \t train_loss 0.1022 \t val_acc 0.2467 \t macro_f1 0.4309\n",
      "Epoch 05 \t train_loss 0.1014 \t val_acc 0.2513 \t macro_f1 0.4377\n",
      "Epoch 06 \t train_loss 0.1009 \t val_acc 0.2538 \t macro_f1 0.4439\n",
      "Epoch 07 \t train_loss 0.1005 \t val_acc 0.2585 \t macro_f1 0.4497\n",
      "Epoch 08 \t train_loss 0.1001 \t val_acc 0.2613 \t macro_f1 0.4544\n",
      "Epoch 09 \t train_loss 0.0998 \t val_acc 0.2613 \t macro_f1 0.4564\n",
      "Epoch 10 \t train_loss 0.0995 \t val_acc 0.2645 \t macro_f1 0.4605\n",
      "Epoch 11 \t train_loss 0.0993 \t val_acc 0.2641 \t macro_f1 0.4602\n",
      "Epoch 12 \t train_loss 0.0990 \t val_acc 0.2659 \t macro_f1 0.4601\n",
      "Epoch 13 \t train_loss 0.0990 \t val_acc 0.2680 \t macro_f1 0.4656\n",
      "Epoch 14 \t train_loss 0.0988 \t val_acc 0.2714 \t macro_f1 0.4723\n",
      "Epoch 15 \t train_loss 0.0986 \t val_acc 0.2676 \t macro_f1 0.4633\n",
      "Epoch 16 \t train_loss 0.0984 \t val_acc 0.2720 \t macro_f1 0.4739\n",
      "Epoch 17 \t train_loss 0.0983 \t val_acc 0.2712 \t macro_f1 0.4698\n",
      "Epoch 18 \t train_loss 0.0981 \t val_acc 0.2716 \t macro_f1 0.4705\n",
      "Epoch 19 \t train_loss 0.0980 \t val_acc 0.2728 \t macro_f1 0.4749\n",
      "Epoch 20 \t train_loss 0.0979 \t val_acc 0.2737 \t macro_f1 0.4758\n",
      "Epoch 21 \t train_loss 0.0977 \t val_acc 0.2727 \t macro_f1 0.4745\n",
      "Epoch 22 \t train_loss 0.0976 \t val_acc 0.2748 \t macro_f1 0.4766\n",
      "Epoch 23 \t train_loss 0.0975 \t val_acc 0.2771 \t macro_f1 0.4814\n",
      "Epoch 24 \t train_loss 0.0975 \t val_acc 0.2756 \t macro_f1 0.4765\n",
      "Epoch 25 \t train_loss 0.0973 \t val_acc 0.2767 \t macro_f1 0.4789\n",
      "Epoch 26 \t train_loss 0.0972 \t val_acc 0.2768 \t macro_f1 0.4800\n",
      "Epoch 27 \t train_loss 0.0972 \t val_acc 0.2761 \t macro_f1 0.4809\n",
      "Epoch 28 \t train_loss 0.0971 \t val_acc 0.2774 \t macro_f1 0.4793\n",
      "Epoch 29 \t train_loss 0.0971 \t val_acc 0.2786 \t macro_f1 0.4822\n",
      "Epoch 30 \t train_loss 0.0969 \t val_acc 0.2784 \t macro_f1 0.4831\n",
      "Epoch 31 \t train_loss 0.0969 \t val_acc 0.2775 \t macro_f1 0.4808\n",
      "Epoch 32 \t train_loss 0.0968 \t val_acc 0.2776 \t macro_f1 0.4828\n",
      "Epoch 33 \t train_loss 0.0967 \t val_acc 0.2794 \t macro_f1 0.4830\n",
      "Epoch 34 \t train_loss 0.0967 \t val_acc 0.2790 \t macro_f1 0.4832\n",
      "Epoch 35 \t train_loss 0.0967 \t val_acc 0.2798 \t macro_f1 0.4839\n",
      "Epoch 36 \t train_loss 0.0966 \t val_acc 0.2791 \t macro_f1 0.4831\n",
      "Epoch 37 \t train_loss 0.0966 \t val_acc 0.2793 \t macro_f1 0.4839\n",
      "Epoch 38 \t train_loss 0.0965 \t val_acc 0.2798 \t macro_f1 0.4845\n",
      "Epoch 39 \t train_loss 0.0965 \t val_acc 0.2801 \t macro_f1 0.4847\n",
      "Epoch 40 \t train_loss 0.0965 \t val_acc 0.2799 \t macro_f1 0.4844\n",
      "Epoch 41 \t train_loss 0.0964 \t val_acc 0.2800 \t macro_f1 0.4850\n",
      "Epoch 42 \t train_loss 0.0964 \t val_acc 0.2802 \t macro_f1 0.4847\n",
      "Epoch 43 \t train_loss 0.0964 \t val_acc 0.2807 \t macro_f1 0.4855\n",
      "Epoch 44 \t train_loss 0.0964 \t val_acc 0.2804 \t macro_f1 0.4847\n",
      "Epoch 45 \t train_loss 0.0964 \t val_acc 0.2804 \t macro_f1 0.4851\n",
      "Epoch 46 \t train_loss 0.0963 \t val_acc 0.2805 \t macro_f1 0.4852\n",
      "Epoch 47 \t train_loss 0.0963 \t val_acc 0.2806 \t macro_f1 0.4852\n",
      "Epoch 48 \t train_loss 0.0964 \t val_acc 0.2806 \t macro_f1 0.4853\n",
      "Epoch 49 \t train_loss 0.0963 \t val_acc 0.2806 \t macro_f1 0.4853\n",
      "Epoch 50 \t train_loss 0.0963 \t val_acc 0.2806 \t macro_f1 0.4853\n",
      "Epoch 51 \t train_loss 0.0963 \t val_acc 0.2805 \t macro_f1 0.4853\n",
      "Epoch 52 \t train_loss 0.0963 \t val_acc 0.2806 \t macro_f1 0.4854\n",
      "Epoch 53 \t train_loss 0.0963 \t val_acc 0.2807 \t macro_f1 0.4854\n",
      "Epoch 54 \t train_loss 0.0963 \t val_acc 0.2808 \t macro_f1 0.4853\n",
      "Epoch 55 \t train_loss 0.0963 \t val_acc 0.2805 \t macro_f1 0.4854\n",
      "Epoch 56 \t train_loss 0.0963 \t val_acc 0.2808 \t macro_f1 0.4852\n",
      "Epoch 57 \t train_loss 0.0964 \t val_acc 0.2809 \t macro_f1 0.4860\n",
      "Epoch 58 \t train_loss 0.0964 \t val_acc 0.2808 \t macro_f1 0.4857\n",
      "Epoch 59 \t train_loss 0.0963 \t val_acc 0.2811 \t macro_f1 0.4861\n",
      "Epoch 60 \t train_loss 0.0963 \t val_acc 0.2809 \t macro_f1 0.4853\n",
      "Epoch 61 \t train_loss 0.0964 \t val_acc 0.2804 \t macro_f1 0.4851\n",
      "Epoch 62 \t train_loss 0.0964 \t val_acc 0.2806 \t macro_f1 0.4850\n",
      "Epoch 63 \t train_loss 0.0964 \t val_acc 0.2804 \t macro_f1 0.4848\n",
      "Epoch 64 \t train_loss 0.0964 \t val_acc 0.2813 \t macro_f1 0.4864\n",
      "Epoch 65 \t train_loss 0.0963 \t val_acc 0.2806 \t macro_f1 0.4852\n",
      "Epoch 66 \t train_loss 0.0964 \t val_acc 0.2811 \t macro_f1 0.4869\n",
      "Epoch 67 \t train_loss 0.0963 \t val_acc 0.2820 \t macro_f1 0.4877\n",
      "Epoch 68 \t train_loss 0.0963 \t val_acc 0.2818 \t macro_f1 0.4871\n",
      "Epoch 69 \t train_loss 0.0964 \t val_acc 0.2797 \t macro_f1 0.4862\n",
      "Epoch 70 \t train_loss 0.0964 \t val_acc 0.2810 \t macro_f1 0.4866\n",
      "Epoch 71 \t train_loss 0.0963 \t val_acc 0.2814 \t macro_f1 0.4873\n",
      "Epoch 72 \t train_loss 0.0963 \t val_acc 0.2817 \t macro_f1 0.4875\n",
      "Epoch 73 \t train_loss 0.0963 \t val_acc 0.2806 \t macro_f1 0.4870\n",
      "Epoch 74 \t train_loss 0.0963 \t val_acc 0.2817 \t macro_f1 0.4893\n",
      "Epoch 75 \t train_loss 0.0963 \t val_acc 0.2830 \t macro_f1 0.4885\n",
      "Epoch 76 \t train_loss 0.0963 \t val_acc 0.2827 \t macro_f1 0.4883\n",
      "Epoch 77 \t train_loss 0.0963 \t val_acc 0.2811 \t macro_f1 0.4873\n",
      "Epoch 78 \t train_loss 0.0963 \t val_acc 0.2823 \t macro_f1 0.4893\n",
      "Epoch 79 \t train_loss 0.0962 \t val_acc 0.2839 \t macro_f1 0.4914\n",
      "Epoch 80 \t train_loss 0.0962 \t val_acc 0.2824 \t macro_f1 0.4893\n",
      "Epoch 81 \t train_loss 0.0963 \t val_acc 0.2823 \t macro_f1 0.4889\n",
      "Epoch 82 \t train_loss 0.0963 \t val_acc 0.2819 \t macro_f1 0.4902\n",
      "Epoch 83 \t train_loss 0.0962 \t val_acc 0.2816 \t macro_f1 0.4886\n",
      "Epoch 84 \t train_loss 0.0962 \t val_acc 0.2816 \t macro_f1 0.4877\n",
      "Epoch 85 \t train_loss 0.0962 \t val_acc 0.2824 \t macro_f1 0.4891\n",
      "Epoch 86 \t train_loss 0.0962 \t val_acc 0.2810 \t macro_f1 0.4862\n",
      "Epoch 87 \t train_loss 0.0962 \t val_acc 0.2828 \t macro_f1 0.4901\n",
      "Epoch 88 \t train_loss 0.0962 \t val_acc 0.2833 \t macro_f1 0.4929\n",
      "Epoch 89 \t train_loss 0.0962 \t val_acc 0.2825 \t macro_f1 0.4929\n",
      "Early stopping at epoch 89 (no improvement for 10 epochs)\n",
      "======\n",
      "Best epoch 89 \t val_acc 0.2839 \t macro_f1 0.4914\n",
      "CPU times: user 30min 3s, sys: 1h 4min 7s, total: 1h 34min 11s\n",
      "Wall time: 13min 29s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "run_experiment('mlp', use_img=False, use_txt=True, use_loc=True, use_mixup=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5ee7562c-ca2d-4293-9410-e77a3665ab2c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00 \t train_loss 0.1460 \t val_acc 0.1855 \t macro_f1 0.3081\n",
      "Epoch 01 \t train_loss 0.0989 \t val_acc 0.2288 \t macro_f1 0.3951\n",
      "Epoch 02 \t train_loss 0.0950 \t val_acc 0.2425 \t macro_f1 0.4217\n",
      "Epoch 03 \t train_loss 0.0933 \t val_acc 0.2511 \t macro_f1 0.4381\n",
      "Epoch 04 \t train_loss 0.0922 \t val_acc 0.2567 \t macro_f1 0.4461\n",
      "Epoch 05 \t train_loss 0.0914 \t val_acc 0.2608 \t macro_f1 0.4556\n",
      "Epoch 06 \t train_loss 0.0907 \t val_acc 0.2651 \t macro_f1 0.4626\n",
      "Epoch 07 \t train_loss 0.0903 \t val_acc 0.2684 \t macro_f1 0.4653\n",
      "Epoch 08 \t train_loss 0.0899 \t val_acc 0.2677 \t macro_f1 0.4663\n",
      "Epoch 09 \t train_loss 0.0895 \t val_acc 0.2711 \t macro_f1 0.4707\n",
      "Epoch 10 \t train_loss 0.0892 \t val_acc 0.2732 \t macro_f1 0.4735\n",
      "Epoch 11 \t train_loss 0.0890 \t val_acc 0.2731 \t macro_f1 0.4734\n",
      "Epoch 12 \t train_loss 0.0887 \t val_acc 0.2740 \t macro_f1 0.4759\n",
      "Epoch 13 \t train_loss 0.0884 \t val_acc 0.2756 \t macro_f1 0.4799\n",
      "Epoch 14 \t train_loss 0.0882 \t val_acc 0.2771 \t macro_f1 0.4792\n",
      "Epoch 15 \t train_loss 0.0880 \t val_acc 0.2778 \t macro_f1 0.4828\n",
      "Epoch 16 \t train_loss 0.0878 \t val_acc 0.2782 \t macro_f1 0.4830\n",
      "Epoch 17 \t train_loss 0.0877 \t val_acc 0.2796 \t macro_f1 0.4835\n",
      "Epoch 18 \t train_loss 0.0875 \t val_acc 0.2794 \t macro_f1 0.4835\n",
      "Epoch 19 \t train_loss 0.0874 \t val_acc 0.2792 \t macro_f1 0.4829\n",
      "Epoch 20 \t train_loss 0.0873 \t val_acc 0.2818 \t macro_f1 0.4875\n",
      "Epoch 21 \t train_loss 0.0871 \t val_acc 0.2810 \t macro_f1 0.4870\n",
      "Epoch 22 \t train_loss 0.0870 \t val_acc 0.2831 \t macro_f1 0.4889\n",
      "Epoch 23 \t train_loss 0.0869 \t val_acc 0.2820 \t macro_f1 0.4891\n",
      "Epoch 24 \t train_loss 0.0868 \t val_acc 0.2829 \t macro_f1 0.4903\n",
      "Epoch 25 \t train_loss 0.0867 \t val_acc 0.2825 \t macro_f1 0.4895\n",
      "Epoch 26 \t train_loss 0.0866 \t val_acc 0.2832 \t macro_f1 0.4900\n",
      "Epoch 27 \t train_loss 0.0865 \t val_acc 0.2844 \t macro_f1 0.4909\n",
      "Epoch 28 \t train_loss 0.0864 \t val_acc 0.2854 \t macro_f1 0.4920\n",
      "Epoch 29 \t train_loss 0.0863 \t val_acc 0.2852 \t macro_f1 0.4919\n",
      "Epoch 30 \t train_loss 0.0863 \t val_acc 0.2851 \t macro_f1 0.4914\n",
      "Epoch 31 \t train_loss 0.0862 \t val_acc 0.2848 \t macro_f1 0.4906\n",
      "Epoch 32 \t train_loss 0.0861 \t val_acc 0.2854 \t macro_f1 0.4920\n",
      "Epoch 33 \t train_loss 0.0860 \t val_acc 0.2861 \t macro_f1 0.4930\n",
      "Epoch 34 \t train_loss 0.0860 \t val_acc 0.2858 \t macro_f1 0.4924\n",
      "Epoch 35 \t train_loss 0.0860 \t val_acc 0.2861 \t macro_f1 0.4928\n",
      "Epoch 36 \t train_loss 0.0859 \t val_acc 0.2864 \t macro_f1 0.4922\n",
      "Epoch 37 \t train_loss 0.0858 \t val_acc 0.2862 \t macro_f1 0.4936\n",
      "Epoch 38 \t train_loss 0.0857 \t val_acc 0.2868 \t macro_f1 0.4942\n",
      "Epoch 39 \t train_loss 0.0857 \t val_acc 0.2870 \t macro_f1 0.4939\n",
      "Epoch 40 \t train_loss 0.0857 \t val_acc 0.2870 \t macro_f1 0.4935\n",
      "Epoch 41 \t train_loss 0.0857 \t val_acc 0.2861 \t macro_f1 0.4931\n",
      "Epoch 42 \t train_loss 0.0856 \t val_acc 0.2871 \t macro_f1 0.4941\n",
      "Epoch 43 \t train_loss 0.0856 \t val_acc 0.2869 \t macro_f1 0.4935\n",
      "Epoch 44 \t train_loss 0.0856 \t val_acc 0.2869 \t macro_f1 0.4936\n",
      "Epoch 45 \t train_loss 0.0856 \t val_acc 0.2870 \t macro_f1 0.4939\n",
      "Epoch 46 \t train_loss 0.0856 \t val_acc 0.2869 \t macro_f1 0.4941\n",
      "Epoch 47 \t train_loss 0.0856 \t val_acc 0.2869 \t macro_f1 0.4941\n",
      "Epoch 48 \t train_loss 0.0855 \t val_acc 0.2870 \t macro_f1 0.4943\n",
      "Epoch 49 \t train_loss 0.0855 \t val_acc 0.2870 \t macro_f1 0.4943\n",
      "Epoch 50 \t train_loss 0.0856 \t val_acc 0.2870 \t macro_f1 0.4943\n",
      "Epoch 51 \t train_loss 0.0855 \t val_acc 0.2870 \t macro_f1 0.4942\n",
      "Epoch 52 \t train_loss 0.0855 \t val_acc 0.2870 \t macro_f1 0.4943\n",
      "Early stopping at epoch 52 (no improvement for 10 epochs)\n",
      "======\n",
      "Best epoch 52 \t val_acc 0.2871 \t macro_f1 0.4941\n",
      "CPU times: user 22min 20s, sys: 30min 49s, total: 53min 9s\n",
      "Wall time: 7min 10s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "run_experiment('mlp', use_img=False, use_txt=True, use_loc=True, use_mixup=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b79c3536-82c0-43b5-bc29-a9583bbf0347",
   "metadata": {},
   "source": [
    "### Image + title + location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "35dbac38-ad3a-4380-bbaf-dcc518f69eb1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00 \t train_loss 0.1194 \t val_acc 0.2688 \t macro_f1 0.4964\n",
      "Epoch 01 \t train_loss 0.0897 \t val_acc 0.2941 \t macro_f1 0.5418\n",
      "Epoch 02 \t train_loss 0.0874 \t val_acc 0.3039 \t macro_f1 0.5561\n",
      "Epoch 03 \t train_loss 0.0864 \t val_acc 0.3092 \t macro_f1 0.5689\n",
      "Epoch 04 \t train_loss 0.0856 \t val_acc 0.3115 \t macro_f1 0.5737\n",
      "Epoch 05 \t train_loss 0.0853 \t val_acc 0.3145 \t macro_f1 0.5790\n",
      "Epoch 06 \t train_loss 0.0850 \t val_acc 0.3175 \t macro_f1 0.5811\n",
      "Epoch 07 \t train_loss 0.0847 \t val_acc 0.3184 \t macro_f1 0.5828\n",
      "Epoch 08 \t train_loss 0.0845 \t val_acc 0.3208 \t macro_f1 0.5861\n",
      "Epoch 09 \t train_loss 0.0843 \t val_acc 0.3208 \t macro_f1 0.5850\n",
      "Epoch 10 \t train_loss 0.0842 \t val_acc 0.3213 \t macro_f1 0.5878\n",
      "Epoch 11 \t train_loss 0.0840 \t val_acc 0.3223 \t macro_f1 0.5887\n",
      "Epoch 12 \t train_loss 0.0840 \t val_acc 0.3228 \t macro_f1 0.5912\n",
      "Epoch 13 \t train_loss 0.0839 \t val_acc 0.3233 \t macro_f1 0.5912\n",
      "Epoch 14 \t train_loss 0.0839 \t val_acc 0.3246 \t macro_f1 0.5919\n",
      "Epoch 15 \t train_loss 0.0838 \t val_acc 0.3247 \t macro_f1 0.5924\n",
      "Epoch 16 \t train_loss 0.0837 \t val_acc 0.3250 \t macro_f1 0.5910\n",
      "Epoch 17 \t train_loss 0.0837 \t val_acc 0.3248 \t macro_f1 0.5916\n",
      "Epoch 18 \t train_loss 0.0837 \t val_acc 0.3239 \t macro_f1 0.5924\n",
      "Epoch 19 \t train_loss 0.0836 \t val_acc 0.3254 \t macro_f1 0.5933\n",
      "Epoch 20 \t train_loss 0.0836 \t val_acc 0.3247 \t macro_f1 0.5950\n",
      "Epoch 21 \t train_loss 0.0835 \t val_acc 0.3268 \t macro_f1 0.5945\n",
      "Epoch 22 \t train_loss 0.0835 \t val_acc 0.3254 \t macro_f1 0.5947\n",
      "Epoch 23 \t train_loss 0.0834 \t val_acc 0.3263 \t macro_f1 0.5939\n",
      "Epoch 24 \t train_loss 0.0834 \t val_acc 0.3253 \t macro_f1 0.5966\n",
      "Epoch 25 \t train_loss 0.0834 \t val_acc 0.3266 \t macro_f1 0.5944\n",
      "Epoch 26 \t train_loss 0.0833 \t val_acc 0.3266 \t macro_f1 0.5951\n",
      "Epoch 27 \t train_loss 0.0833 \t val_acc 0.3264 \t macro_f1 0.5946\n",
      "Epoch 28 \t train_loss 0.0833 \t val_acc 0.3264 \t macro_f1 0.5943\n",
      "Epoch 29 \t train_loss 0.0833 \t val_acc 0.3263 \t macro_f1 0.5962\n",
      "Epoch 30 \t train_loss 0.0832 \t val_acc 0.3272 \t macro_f1 0.5935\n",
      "Epoch 31 \t train_loss 0.0832 \t val_acc 0.3265 \t macro_f1 0.5968\n",
      "Epoch 32 \t train_loss 0.0832 \t val_acc 0.3265 \t macro_f1 0.5962\n",
      "Epoch 33 \t train_loss 0.0832 \t val_acc 0.3278 \t macro_f1 0.5970\n",
      "Epoch 34 \t train_loss 0.0831 \t val_acc 0.3270 \t macro_f1 0.5961\n",
      "Epoch 35 \t train_loss 0.0831 \t val_acc 0.3274 \t macro_f1 0.5967\n",
      "Epoch 36 \t train_loss 0.0831 \t val_acc 0.3271 \t macro_f1 0.5974\n",
      "Epoch 37 \t train_loss 0.0831 \t val_acc 0.3271 \t macro_f1 0.5981\n",
      "Epoch 38 \t train_loss 0.0831 \t val_acc 0.3271 \t macro_f1 0.5960\n",
      "Epoch 39 \t train_loss 0.0830 \t val_acc 0.3268 \t macro_f1 0.5964\n",
      "Epoch 40 \t train_loss 0.0830 \t val_acc 0.3272 \t macro_f1 0.5965\n",
      "Epoch 41 \t train_loss 0.0830 \t val_acc 0.3270 \t macro_f1 0.5967\n",
      "Epoch 42 \t train_loss 0.0830 \t val_acc 0.3277 \t macro_f1 0.5970\n",
      "Epoch 43 \t train_loss 0.0830 \t val_acc 0.3276 \t macro_f1 0.5968\n",
      "Early stopping at epoch 43 (no improvement for 10 epochs)\n",
      "======\n",
      "Best epoch 43 \t val_acc 0.3278 \t macro_f1 0.5970\n",
      "CPU times: user 13min 59s, sys: 29min 1s, total: 43min\n",
      "Wall time: 6min 12s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "run_experiment('linear', use_img=True, use_txt=True, use_loc=True, use_mixup=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4880d3b8-b8fd-4d8d-86ec-823723b9e983",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00 \t train_loss 0.1088 \t val_acc 0.2659 \t macro_f1 0.4828\n",
      "Epoch 01 \t train_loss 0.0778 \t val_acc 0.2921 \t macro_f1 0.5296\n",
      "Epoch 02 \t train_loss 0.0752 \t val_acc 0.3018 \t macro_f1 0.5455\n",
      "Epoch 03 \t train_loss 0.0739 \t val_acc 0.3078 \t macro_f1 0.5544\n",
      "Epoch 04 \t train_loss 0.0731 \t val_acc 0.3108 \t macro_f1 0.5600\n",
      "Epoch 05 \t train_loss 0.0726 \t val_acc 0.3135 \t macro_f1 0.5650\n",
      "Epoch 06 \t train_loss 0.0722 \t val_acc 0.3161 \t macro_f1 0.5693\n",
      "Epoch 07 \t train_loss 0.0719 \t val_acc 0.3183 \t macro_f1 0.5685\n",
      "Epoch 08 \t train_loss 0.0716 \t val_acc 0.3196 \t macro_f1 0.5703\n",
      "Epoch 09 \t train_loss 0.0714 \t val_acc 0.3204 \t macro_f1 0.5728\n",
      "Epoch 10 \t train_loss 0.0712 \t val_acc 0.3206 \t macro_f1 0.5746\n",
      "Epoch 11 \t train_loss 0.0711 \t val_acc 0.3218 \t macro_f1 0.5784\n",
      "Epoch 12 \t train_loss 0.0710 \t val_acc 0.3220 \t macro_f1 0.5784\n",
      "Epoch 13 \t train_loss 0.0709 \t val_acc 0.3231 \t macro_f1 0.5765\n",
      "Epoch 14 \t train_loss 0.0708 \t val_acc 0.3235 \t macro_f1 0.5794\n",
      "Epoch 15 \t train_loss 0.0707 \t val_acc 0.3230 \t macro_f1 0.5800\n",
      "Epoch 16 \t train_loss 0.0706 \t val_acc 0.3237 \t macro_f1 0.5809\n",
      "Epoch 17 \t train_loss 0.0706 \t val_acc 0.3244 \t macro_f1 0.5817\n",
      "Epoch 18 \t train_loss 0.0705 \t val_acc 0.3241 \t macro_f1 0.5815\n",
      "Epoch 19 \t train_loss 0.0705 \t val_acc 0.3253 \t macro_f1 0.5823\n",
      "Epoch 20 \t train_loss 0.0704 \t val_acc 0.3241 \t macro_f1 0.5817\n",
      "Epoch 21 \t train_loss 0.0704 \t val_acc 0.3244 \t macro_f1 0.5810\n",
      "Epoch 22 \t train_loss 0.0703 \t val_acc 0.3253 \t macro_f1 0.5829\n",
      "Epoch 23 \t train_loss 0.0703 \t val_acc 0.3249 \t macro_f1 0.5813\n",
      "Epoch 24 \t train_loss 0.0702 \t val_acc 0.3257 \t macro_f1 0.5833\n",
      "Epoch 25 \t train_loss 0.0702 \t val_acc 0.3261 \t macro_f1 0.5862\n",
      "Epoch 26 \t train_loss 0.0702 \t val_acc 0.3254 \t macro_f1 0.5821\n",
      "Epoch 27 \t train_loss 0.0701 \t val_acc 0.3255 \t macro_f1 0.5809\n",
      "Epoch 28 \t train_loss 0.0701 \t val_acc 0.3250 \t macro_f1 0.5828\n",
      "Epoch 29 \t train_loss 0.0701 \t val_acc 0.3261 \t macro_f1 0.5827\n",
      "Epoch 30 \t train_loss 0.0700 \t val_acc 0.3254 \t macro_f1 0.5836\n",
      "Epoch 31 \t train_loss 0.0700 \t val_acc 0.3257 \t macro_f1 0.5841\n",
      "Epoch 32 \t train_loss 0.0700 \t val_acc 0.3263 \t macro_f1 0.5838\n",
      "Epoch 33 \t train_loss 0.0699 \t val_acc 0.3265 \t macro_f1 0.5851\n",
      "Epoch 34 \t train_loss 0.0699 \t val_acc 0.3265 \t macro_f1 0.5826\n",
      "Epoch 35 \t train_loss 0.0699 \t val_acc 0.3266 \t macro_f1 0.5845\n",
      "Epoch 36 \t train_loss 0.0698 \t val_acc 0.3259 \t macro_f1 0.5835\n",
      "Epoch 37 \t train_loss 0.0698 \t val_acc 0.3262 \t macro_f1 0.5843\n",
      "Epoch 38 \t train_loss 0.0698 \t val_acc 0.3263 \t macro_f1 0.5842\n",
      "Epoch 39 \t train_loss 0.0698 \t val_acc 0.3264 \t macro_f1 0.5846\n",
      "Epoch 40 \t train_loss 0.0698 \t val_acc 0.3265 \t macro_f1 0.5840\n",
      "Epoch 41 \t train_loss 0.0697 \t val_acc 0.3267 \t macro_f1 0.5843\n",
      "Epoch 42 \t train_loss 0.0697 \t val_acc 0.3265 \t macro_f1 0.5840\n",
      "Epoch 43 \t train_loss 0.0697 \t val_acc 0.3268 \t macro_f1 0.5849\n",
      "Epoch 44 \t train_loss 0.0697 \t val_acc 0.3266 \t macro_f1 0.5842\n",
      "Epoch 45 \t train_loss 0.0697 \t val_acc 0.3266 \t macro_f1 0.5842\n",
      "Epoch 46 \t train_loss 0.0697 \t val_acc 0.3265 \t macro_f1 0.5842\n",
      "Epoch 47 \t train_loss 0.0697 \t val_acc 0.3265 \t macro_f1 0.5842\n",
      "Epoch 48 \t train_loss 0.0696 \t val_acc 0.3265 \t macro_f1 0.5842\n",
      "Epoch 49 \t train_loss 0.0696 \t val_acc 0.3265 \t macro_f1 0.5842\n",
      "Epoch 50 \t train_loss 0.0696 \t val_acc 0.3265 \t macro_f1 0.5842\n",
      "Epoch 51 \t train_loss 0.0696 \t val_acc 0.3265 \t macro_f1 0.5843\n",
      "Epoch 52 \t train_loss 0.0696 \t val_acc 0.3265 \t macro_f1 0.5843\n",
      "Epoch 53 \t train_loss 0.0697 \t val_acc 0.3265 \t macro_f1 0.5845\n",
      "Early stopping at epoch 53 (no improvement for 10 epochs)\n",
      "======\n",
      "Best epoch 53 \t val_acc 0.3268 \t macro_f1 0.5849\n",
      "CPU times: user 20min 17s, sys: 28min 35s, total: 48min 52s\n",
      "Wall time: 6min 44s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "run_experiment('linear', use_img=True, use_txt=True, use_loc=True, use_mixup=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0f5c01bd-b0a8-48b3-b57b-68c2da9bad8b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00 \t train_loss 0.1408 \t val_acc 0.1920 \t macro_f1 0.2995\n",
      "Epoch 01 \t train_loss 0.1006 \t val_acc 0.2449 \t macro_f1 0.4040\n",
      "Epoch 02 \t train_loss 0.0967 \t val_acc 0.2616 \t macro_f1 0.4380\n",
      "Epoch 03 \t train_loss 0.0949 \t val_acc 0.2721 \t macro_f1 0.4578\n",
      "Epoch 04 \t train_loss 0.0939 \t val_acc 0.2787 \t macro_f1 0.4693\n",
      "Epoch 05 \t train_loss 0.0931 \t val_acc 0.2822 \t macro_f1 0.4771\n",
      "Epoch 06 \t train_loss 0.0926 \t val_acc 0.2867 \t macro_f1 0.4825\n",
      "Epoch 07 \t train_loss 0.0921 \t val_acc 0.2909 \t macro_f1 0.4893\n",
      "Epoch 08 \t train_loss 0.0917 \t val_acc 0.2938 \t macro_f1 0.4917\n",
      "Epoch 09 \t train_loss 0.0914 \t val_acc 0.2943 \t macro_f1 0.4926\n",
      "Epoch 10 \t train_loss 0.0911 \t val_acc 0.2987 \t macro_f1 0.5011\n",
      "Epoch 11 \t train_loss 0.0909 \t val_acc 0.2980 \t macro_f1 0.4970\n",
      "Epoch 12 \t train_loss 0.0906 \t val_acc 0.3013 \t macro_f1 0.5031\n",
      "Epoch 13 \t train_loss 0.0904 \t val_acc 0.3014 \t macro_f1 0.5041\n",
      "Epoch 14 \t train_loss 0.0903 \t val_acc 0.3007 \t macro_f1 0.5052\n",
      "Epoch 15 \t train_loss 0.0902 \t val_acc 0.3018 \t macro_f1 0.5054\n",
      "Epoch 16 \t train_loss 0.0902 \t val_acc 0.3029 \t macro_f1 0.5069\n",
      "Epoch 17 \t train_loss 0.0900 \t val_acc 0.3031 \t macro_f1 0.5105\n",
      "Epoch 18 \t train_loss 0.0898 \t val_acc 0.3038 \t macro_f1 0.5107\n",
      "Epoch 19 \t train_loss 0.0897 \t val_acc 0.3033 \t macro_f1 0.5057\n",
      "Epoch 20 \t train_loss 0.0896 \t val_acc 0.3038 \t macro_f1 0.5122\n",
      "Epoch 21 \t train_loss 0.0895 \t val_acc 0.3067 \t macro_f1 0.5103\n",
      "Epoch 22 \t train_loss 0.0894 \t val_acc 0.3067 \t macro_f1 0.5119\n",
      "Epoch 23 \t train_loss 0.0893 \t val_acc 0.3089 \t macro_f1 0.5155\n",
      "Epoch 24 \t train_loss 0.0891 \t val_acc 0.3076 \t macro_f1 0.5143\n",
      "Epoch 25 \t train_loss 0.0890 \t val_acc 0.3069 \t macro_f1 0.5150\n",
      "Epoch 26 \t train_loss 0.0890 \t val_acc 0.3079 \t macro_f1 0.5147\n",
      "Epoch 27 \t train_loss 0.0888 \t val_acc 0.3103 \t macro_f1 0.5191\n",
      "Epoch 28 \t train_loss 0.0888 \t val_acc 0.3096 \t macro_f1 0.5179\n",
      "Epoch 29 \t train_loss 0.0888 \t val_acc 0.3094 \t macro_f1 0.5159\n",
      "Epoch 30 \t train_loss 0.0887 \t val_acc 0.3092 \t macro_f1 0.5167\n",
      "Epoch 31 \t train_loss 0.0886 \t val_acc 0.3097 \t macro_f1 0.5158\n",
      "Epoch 32 \t train_loss 0.0885 \t val_acc 0.3105 \t macro_f1 0.5188\n",
      "Epoch 33 \t train_loss 0.0885 \t val_acc 0.3117 \t macro_f1 0.5208\n",
      "Epoch 34 \t train_loss 0.0884 \t val_acc 0.3115 \t macro_f1 0.5203\n",
      "Epoch 35 \t train_loss 0.0884 \t val_acc 0.3099 \t macro_f1 0.5183\n",
      "Epoch 36 \t train_loss 0.0883 \t val_acc 0.3110 \t macro_f1 0.5182\n",
      "Epoch 37 \t train_loss 0.0883 \t val_acc 0.3108 \t macro_f1 0.5193\n",
      "Epoch 38 \t train_loss 0.0882 \t val_acc 0.3110 \t macro_f1 0.5197\n",
      "Epoch 39 \t train_loss 0.0882 \t val_acc 0.3113 \t macro_f1 0.5197\n",
      "Epoch 40 \t train_loss 0.0882 \t val_acc 0.3115 \t macro_f1 0.5205\n",
      "Epoch 41 \t train_loss 0.0881 \t val_acc 0.3120 \t macro_f1 0.5215\n",
      "Epoch 42 \t train_loss 0.0881 \t val_acc 0.3119 \t macro_f1 0.5215\n",
      "Epoch 43 \t train_loss 0.0881 \t val_acc 0.3118 \t macro_f1 0.5209\n",
      "Epoch 44 \t train_loss 0.0881 \t val_acc 0.3120 \t macro_f1 0.5211\n",
      "Epoch 45 \t train_loss 0.0881 \t val_acc 0.3119 \t macro_f1 0.5210\n",
      "Epoch 46 \t train_loss 0.0881 \t val_acc 0.3120 \t macro_f1 0.5210\n",
      "Epoch 47 \t train_loss 0.0880 \t val_acc 0.3120 \t macro_f1 0.5212\n",
      "Epoch 48 \t train_loss 0.0880 \t val_acc 0.3121 \t macro_f1 0.5211\n",
      "Epoch 49 \t train_loss 0.0881 \t val_acc 0.3121 \t macro_f1 0.5212\n",
      "Epoch 50 \t train_loss 0.0880 \t val_acc 0.3121 \t macro_f1 0.5212\n",
      "Epoch 51 \t train_loss 0.0881 \t val_acc 0.3121 \t macro_f1 0.5212\n",
      "Epoch 52 \t train_loss 0.0880 \t val_acc 0.3120 \t macro_f1 0.5210\n",
      "Epoch 53 \t train_loss 0.0880 \t val_acc 0.3121 \t macro_f1 0.5214\n",
      "Epoch 54 \t train_loss 0.0880 \t val_acc 0.3123 \t macro_f1 0.5212\n",
      "Epoch 55 \t train_loss 0.0881 \t val_acc 0.3121 \t macro_f1 0.5212\n",
      "Epoch 56 \t train_loss 0.0881 \t val_acc 0.3118 \t macro_f1 0.5210\n",
      "Epoch 57 \t train_loss 0.0881 \t val_acc 0.3120 \t macro_f1 0.5215\n",
      "Epoch 58 \t train_loss 0.0881 \t val_acc 0.3126 \t macro_f1 0.5224\n",
      "Epoch 59 \t train_loss 0.0880 \t val_acc 0.3120 \t macro_f1 0.5221\n",
      "Epoch 60 \t train_loss 0.0881 \t val_acc 0.3116 \t macro_f1 0.5205\n",
      "Epoch 61 \t train_loss 0.0881 \t val_acc 0.3123 \t macro_f1 0.5220\n",
      "Epoch 62 \t train_loss 0.0882 \t val_acc 0.3115 \t macro_f1 0.5214\n",
      "Epoch 63 \t train_loss 0.0881 \t val_acc 0.3122 \t macro_f1 0.5215\n",
      "Epoch 64 \t train_loss 0.0881 \t val_acc 0.3127 \t macro_f1 0.5225\n",
      "Epoch 65 \t train_loss 0.0880 \t val_acc 0.3119 \t macro_f1 0.5216\n",
      "Epoch 66 \t train_loss 0.0881 \t val_acc 0.3134 \t macro_f1 0.5236\n",
      "Epoch 67 \t train_loss 0.0881 \t val_acc 0.3113 \t macro_f1 0.5202\n",
      "Epoch 68 \t train_loss 0.0881 \t val_acc 0.3109 \t macro_f1 0.5199\n",
      "Epoch 69 \t train_loss 0.0881 \t val_acc 0.3124 \t macro_f1 0.5232\n",
      "Epoch 70 \t train_loss 0.0881 \t val_acc 0.3130 \t macro_f1 0.5236\n",
      "Epoch 71 \t train_loss 0.0880 \t val_acc 0.3116 \t macro_f1 0.5232\n",
      "Epoch 72 \t train_loss 0.0881 \t val_acc 0.3104 \t macro_f1 0.5222\n",
      "Epoch 73 \t train_loss 0.0881 \t val_acc 0.3135 \t macro_f1 0.5266\n",
      "Epoch 74 \t train_loss 0.0882 \t val_acc 0.3117 \t macro_f1 0.5216\n",
      "Epoch 75 \t train_loss 0.0881 \t val_acc 0.3131 \t macro_f1 0.5242\n",
      "Epoch 76 \t train_loss 0.0881 \t val_acc 0.3124 \t macro_f1 0.5250\n",
      "Epoch 77 \t train_loss 0.0881 \t val_acc 0.3119 \t macro_f1 0.5227\n",
      "Epoch 78 \t train_loss 0.0881 \t val_acc 0.3133 \t macro_f1 0.5280\n",
      "Epoch 79 \t train_loss 0.0881 \t val_acc 0.3124 \t macro_f1 0.5220\n",
      "Epoch 80 \t train_loss 0.0880 \t val_acc 0.3118 \t macro_f1 0.5239\n",
      "Epoch 81 \t train_loss 0.0881 \t val_acc 0.3119 \t macro_f1 0.5239\n",
      "Epoch 82 \t train_loss 0.0881 \t val_acc 0.3129 \t macro_f1 0.5241\n",
      "Epoch 83 \t train_loss 0.0880 \t val_acc 0.3117 \t macro_f1 0.5236\n",
      "Early stopping at epoch 83 (no improvement for 10 epochs)\n",
      "======\n",
      "Best epoch 83 \t val_acc 0.3135 \t macro_f1 0.5266\n",
      "CPU times: user 30min 3s, sys: 1h 4min 3s, total: 1h 34min 6s\n",
      "Wall time: 13min 42s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "run_experiment('mlp', use_img=True, use_txt=True, use_loc=True, use_mixup=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a986d62b-78e6-4d6a-9d40-d6d0a6266132",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00 \t train_loss 0.1326 \t val_acc 0.2072 \t macro_f1 0.3263\n",
      "Epoch 01 \t train_loss 0.0891 \t val_acc 0.2557 \t macro_f1 0.4282\n",
      "Epoch 02 \t train_loss 0.0849 \t val_acc 0.2731 \t macro_f1 0.4631\n",
      "Epoch 03 \t train_loss 0.0829 \t val_acc 0.2850 \t macro_f1 0.4812\n",
      "Epoch 04 \t train_loss 0.0816 \t val_acc 0.2937 \t macro_f1 0.4955\n",
      "Epoch 05 \t train_loss 0.0807 \t val_acc 0.3013 \t macro_f1 0.5072\n",
      "Epoch 06 \t train_loss 0.0800 \t val_acc 0.3042 \t macro_f1 0.5111\n",
      "Epoch 07 \t train_loss 0.0795 \t val_acc 0.3081 \t macro_f1 0.5170\n",
      "Epoch 08 \t train_loss 0.0790 \t val_acc 0.3067 \t macro_f1 0.5177\n",
      "Epoch 09 \t train_loss 0.0786 \t val_acc 0.3088 \t macro_f1 0.5161\n",
      "Epoch 10 \t train_loss 0.0783 \t val_acc 0.3136 \t macro_f1 0.5263\n",
      "Epoch 11 \t train_loss 0.0781 \t val_acc 0.3120 \t macro_f1 0.5229\n",
      "Epoch 12 \t train_loss 0.0778 \t val_acc 0.3147 \t macro_f1 0.5278\n",
      "Epoch 13 \t train_loss 0.0776 \t val_acc 0.3168 \t macro_f1 0.5291\n",
      "Epoch 14 \t train_loss 0.0774 \t val_acc 0.3176 \t macro_f1 0.5343\n",
      "Epoch 15 \t train_loss 0.0773 \t val_acc 0.3201 \t macro_f1 0.5352\n",
      "Epoch 16 \t train_loss 0.0771 \t val_acc 0.3183 \t macro_f1 0.5310\n",
      "Epoch 17 \t train_loss 0.0770 \t val_acc 0.3182 \t macro_f1 0.5330\n",
      "Epoch 18 \t train_loss 0.0768 \t val_acc 0.3190 \t macro_f1 0.5342\n",
      "Epoch 19 \t train_loss 0.0767 \t val_acc 0.3196 \t macro_f1 0.5338\n",
      "Epoch 20 \t train_loss 0.0766 \t val_acc 0.3197 \t macro_f1 0.5342\n",
      "Epoch 21 \t train_loss 0.0765 \t val_acc 0.3182 \t macro_f1 0.5324\n",
      "Epoch 22 \t train_loss 0.0764 \t val_acc 0.3218 \t macro_f1 0.5392\n",
      "Epoch 23 \t train_loss 0.0762 \t val_acc 0.3212 \t macro_f1 0.5385\n",
      "Epoch 24 \t train_loss 0.0761 \t val_acc 0.3207 \t macro_f1 0.5372\n",
      "Epoch 25 \t train_loss 0.0761 \t val_acc 0.3209 \t macro_f1 0.5371\n",
      "Epoch 26 \t train_loss 0.0759 \t val_acc 0.3214 \t macro_f1 0.5381\n",
      "Epoch 27 \t train_loss 0.0758 \t val_acc 0.3219 \t macro_f1 0.5387\n",
      "Epoch 28 \t train_loss 0.0758 \t val_acc 0.3227 \t macro_f1 0.5397\n",
      "Epoch 29 \t train_loss 0.0756 \t val_acc 0.3226 \t macro_f1 0.5376\n",
      "Epoch 30 \t train_loss 0.0757 \t val_acc 0.3234 \t macro_f1 0.5390\n",
      "Epoch 31 \t train_loss 0.0756 \t val_acc 0.3236 \t macro_f1 0.5408\n",
      "Epoch 32 \t train_loss 0.0755 \t val_acc 0.3231 \t macro_f1 0.5418\n",
      "Epoch 33 \t train_loss 0.0754 \t val_acc 0.3230 \t macro_f1 0.5397\n",
      "Epoch 34 \t train_loss 0.0754 \t val_acc 0.3230 \t macro_f1 0.5407\n",
      "Epoch 35 \t train_loss 0.0753 \t val_acc 0.3235 \t macro_f1 0.5413\n",
      "Epoch 36 \t train_loss 0.0753 \t val_acc 0.3233 \t macro_f1 0.5413\n",
      "Epoch 37 \t train_loss 0.0752 \t val_acc 0.3236 \t macro_f1 0.5414\n",
      "Epoch 38 \t train_loss 0.0751 \t val_acc 0.3239 \t macro_f1 0.5408\n",
      "Epoch 39 \t train_loss 0.0751 \t val_acc 0.3246 \t macro_f1 0.5420\n",
      "Epoch 40 \t train_loss 0.0751 \t val_acc 0.3242 \t macro_f1 0.5419\n",
      "Epoch 41 \t train_loss 0.0751 \t val_acc 0.3246 \t macro_f1 0.5428\n",
      "Epoch 42 \t train_loss 0.0751 \t val_acc 0.3246 \t macro_f1 0.5427\n",
      "Epoch 43 \t train_loss 0.0750 \t val_acc 0.3247 \t macro_f1 0.5423\n",
      "Epoch 44 \t train_loss 0.0750 \t val_acc 0.3246 \t macro_f1 0.5417\n",
      "Epoch 45 \t train_loss 0.0750 \t val_acc 0.3248 \t macro_f1 0.5426\n",
      "Epoch 46 \t train_loss 0.0750 \t val_acc 0.3249 \t macro_f1 0.5425\n",
      "Epoch 47 \t train_loss 0.0749 \t val_acc 0.3247 \t macro_f1 0.5423\n",
      "Epoch 48 \t train_loss 0.0749 \t val_acc 0.3247 \t macro_f1 0.5423\n",
      "Epoch 49 \t train_loss 0.0750 \t val_acc 0.3248 \t macro_f1 0.5424\n",
      "Epoch 50 \t train_loss 0.0749 \t val_acc 0.3248 \t macro_f1 0.5424\n",
      "Epoch 51 \t train_loss 0.0750 \t val_acc 0.3248 \t macro_f1 0.5423\n",
      "Epoch 52 \t train_loss 0.0749 \t val_acc 0.3248 \t macro_f1 0.5426\n",
      "Epoch 53 \t train_loss 0.0750 \t val_acc 0.3247 \t macro_f1 0.5424\n",
      "Epoch 54 \t train_loss 0.0750 \t val_acc 0.3248 \t macro_f1 0.5423\n",
      "Epoch 55 \t train_loss 0.0750 \t val_acc 0.3248 \t macro_f1 0.5421\n",
      "Epoch 56 \t train_loss 0.0750 \t val_acc 0.3245 \t macro_f1 0.5415\n",
      "Early stopping at epoch 56 (no improvement for 10 epochs)\n",
      "======\n",
      "Best epoch 56 \t val_acc 0.3249 \t macro_f1 0.5425\n",
      "CPU times: user 25min 56s, sys: 34min 19s, total: 1h 16s\n",
      "Wall time: 8min 6s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "run_experiment('mlp', use_img=True, use_txt=True, use_loc=True, use_mixup=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6723109b-cb5d-467d-86f7-2515b952d242",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c306752-e66f-413d-b294-8284b1c7a731",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
